{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **APS1080: Introduction to Reinforcement Learning**\n",
        "---\n",
        "\n",
        "**Student Name:**\n",
        "\n",
        "**Student ID:**\n",
        "\n",
        "---\n",
        "\n",
        "**Exercise 2:** Cart Pole preliminaries and Monte Carlo\n",
        "\n"
      ],
      "metadata": {
        "id": "XukQ-DWEJ1dt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDvssQd64Pf"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5esgX013vPe"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install xvfb"
      ],
      "metadata": {
        "id": "hBDacu1MsiGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbi2xaFo31Sj"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGqXqJxoAsHG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8b44f766-bc73-447f-e19c-f069fa205609"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7eefb4780670>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L4YayzR4FYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "900c3b45-c989-4a87-e2b8-3c109b4ec933"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations that were run: 37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAs5UlEQVR4nO3df3RU9Z3/8ddMkhkSkpkhhMwkkiD+qBgg2AKGWVvXlpSA6Ooav1+1rGCXrxzZ4KliLaZrVewe42rP+qOr8MfuinuOlNYe0UoFiyCh1oiakuWHmgWWNiiZBMHMJIFMkpnP9w+/zLejmDAhzNwJz8c595zMve+5876fkzAv7k+bMcYIAADAQuypbgAAAOCLCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByUhpQnnnmGZ1//vkaNWqUKioq9O6776ayHQAAYBEpCyi//OUvtXz5cj344IP64x//qGnTpqmqqkrt7e2pagkAAFiELVUPC6yoqNDMmTP1r//6r5KkaDSqkpIS3XnnnbrvvvtS0RIAALCIzFR8aG9vrxobG1VbWxubZ7fbVVlZqYaGhi/Vh8NhhcPh2OtoNKpjx45p7NixstlsSekZAACcGWOMOjs7VVxcLLt94IM4KQkon376qSKRiLxeb9x8r9erjz766Ev1dXV1WrlyZbLaAwAAZ9GhQ4c0fvz4AWtSElASVVtbq+XLl8deB4NBlZaW6tChQ3K5XCnsDAAAnK5QKKSSkhLl5eUNWpuSgFJQUKCMjAy1tbXFzW9ra5PP5/tSvdPplNPp/NJ8l8tFQAEAIM2czukZKbmKx+FwaPr06dqyZUtsXjQa1ZYtW+T3+1PREgAAsJCUHeJZvny5Fi1apBkzZujyyy/Xk08+qe7ubn3/+99PVUsAAMAiUhZQbrrpJh05ckQPPPCAAoGALrvsMm3atOlLJ84CAIBzT8rug3ImQqGQ3G63gsEg56AAAJAmEvn+5lk8AADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcggoAADAcoY9oDz00EOy2Wxx06RJk2LLe3p6VFNTo7Fjxyo3N1fV1dVqa2sb7jYAAEAaOyt7UCZPnqzW1tbY9NZbb8WW3X333Xr11Vf14osvqr6+XocPH9YNN9xwNtoAAABpKvOsrDQzUz6f70vzg8Gg/v3f/11r167Vd77zHUnSc889p0svvVTvvPOOZs2adTbaAQAAaeas7EHZt2+fiouLdcEFF2jBggVqaWmRJDU2Nqqvr0+VlZWx2kmTJqm0tFQNDQ1fub5wOKxQKBQ3AQCAkWvYA0pFRYXWrFmjTZs2adWqVTp48KC+9a1vqbOzU4FAQA6HQx6PJ+49Xq9XgUDgK9dZV1cnt9sdm0pKSoa7bQAAYCHDfohn3rx5sZ/Ly8tVUVGhCRMm6Fe/+pWys7OHtM7a2lotX7489joUChFSAAAYwc76ZcYej0df+9rXtH//fvl8PvX29qqjoyOupq2t7ZTnrJzkdDrlcrniJgAAMHKd9YDS1dWlAwcOqKioSNOnT1dWVpa2bNkSW97c3KyWlhb5/f6z3QoAAEgTw36I54c//KGuvfZaTZgwQYcPH9aDDz6ojIwM3XLLLXK73Vq8eLGWL1+u/Px8uVwu3XnnnfL7/VzBAwAAYoY9oHz88ce65ZZbdPToUY0bN07f/OY39c4772jcuHGSpCeeeEJ2u13V1dUKh8OqqqrSs88+O9xtAACANGYzxphUN5GoUCgkt9utYDDI+SgAAKSJRL6/eRYPAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwHAIKAACwnIQDyvbt23XttdequLhYNptNL7/8ctxyY4weeOABFRUVKTs7W5WVldq3b19czbFjx7RgwQK5XC55PB4tXrxYXV1dZ7QhAABg5Eg4oHR3d2vatGl65plnTrn8scce09NPP63Vq1drx44dGj16tKqqqtTT0xOrWbBggfbu3avNmzdrw4YN2r59u5YsWTL0rQAAACOKzRhjhvxmm03r16/X9ddfL+nzvSfFxcW655579MMf/lCSFAwG5fV6tWbNGt1888368MMPVVZWpvfee08zZsyQJG3atElXX321Pv74YxUXFw/6uaFQSG63W8FgUC6Xa6jtAwCAJErk+3tYz0E5ePCgAoGAKisrY/PcbrcqKirU0NAgSWpoaJDH44mFE0mqrKyU3W7Xjh07TrnecDisUCgUNwEAgJFrWANKIBCQJHm93rj5Xq83tiwQCKiwsDBueWZmpvLz82M1X1RXVye32x2bSkpKhrNtAABgMWlxFU9tba2CwWBsOnToUKpbAgAAZ9GwBhSfzydJamtri5vf1tYWW+bz+dTe3h63vL+/X8eOHYvVfJHT6ZTL5YqbAADAyDWsAWXixIny+XzasmVLbF4oFNKOHTvk9/slSX6/Xx0dHWpsbIzVbN26VdFoVBUVFcPZDgAASFOZib6hq6tL+/fvj70+ePCgmpqalJ+fr9LSUt111136p3/6J1188cWaOHGifvKTn6i4uDh2pc+ll16quXPn6vbbb9fq1avV19enZcuW6eabbz6tK3gAAMDIl3BAef/99/Xtb3879nr58uWSpEWLFmnNmjX60Y9+pO7ubi1ZskQdHR365je/qU2bNmnUqFGx97zwwgtatmyZZs+eLbvdrurqaj399NPDsDkAAGAkOKP7oKQK90EBACD9pOw+KAAAAMOBgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACyHgAIAACwn4YCyfft2XXvttSouLpbNZtPLL78ct/y2226TzWaLm+bOnRtXc+zYMS1YsEAul0sej0eLFy9WV1fXGW0IAAAYORIOKN3d3Zo2bZqeeeaZr6yZO3euWltbY9MvfvGLuOULFizQ3r17tXnzZm3YsEHbt2/XkiVLEu8eAACMSJmJvmHevHmaN2/egDVOp1M+n++Uyz788ENt2rRJ7733nmbMmCFJ+vnPf66rr75aP/vZz1RcXJxoSwAAYIQ5K+egbNu2TYWFhbrkkku0dOlSHT16NLasoaFBHo8nFk4kqbKyUna7XTt27Djl+sLhsEKhUNwEAABGrmEPKHPnztV//ud/asuWLfrnf/5n1dfXa968eYpEIpKkQCCgwsLCuPdkZmYqPz9fgUDglOusq6uT2+2OTSUlJcPdNgAAsJCED/EM5uabb479PHXqVJWXl+vCCy/Utm3bNHv27CGts7a2VsuXL4+9DoVChBQAAEaws36Z8QUXXKCCggLt379fkuTz+dTe3h5X09/fr2PHjn3leStOp1MulytuAgAAI9dZDygff/yxjh49qqKiIkmS3+9XR0eHGhsbYzVbt25VNBpVRUXF2W4HAACkgYQP8XR1dcX2hkjSwYMH1dTUpPz8fOXn52vlypWqrq6Wz+fTgQMH9KMf/UgXXXSRqqqqJEmXXnqp5s6dq9tvv12rV69WX1+fli1bpptvvpkreAAAgCTJZowxibxh27Zt+va3v/2l+YsWLdKqVat0/fXXa+fOnero6FBxcbHmzJmjn/70p/J6vbHaY8eOadmyZXr11Vdlt9tVXV2tp59+Wrm5uafVQygUktvtVjAY5HAPAABpIpHv74QDihUQUAAASD+JfH/zLB4AAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5BBQAAGA5CT/NGABGGmOMTDQim80mySbZbJL0/14DSAUCCoBzXn9Pl3atrVXOuAkaXXC+Ro8rVU5BqexZTtkzHLJnZsme6SCwAElEQAFwzusM7FO0v1ddrfvU1brv85k2m0Z5fMr2+DTKU6RRHp/GXny5bDaOjAPJQEABcM779MO3vjzTGPV81qqez1ol7ZRkU/6FM2TLIKAAycBfGoBzXlf7wVS3AOALCCgAzmm9XZ9JJjpoXclf/S/Z7PyTCSQLf20AzmmhTz5UNNI/aF1e0dckcZIskCwEFADntCMf/UEm0jdonT0ji6t4gCQioAA4Z0Uj/ad1eCev+BJlOHOS0BGAkwgoAM5ZvV2fKdIXHrTOM2GaMp2jk9ARgJMIKADOWUc+3K4Tn30yaF1Wjks2e0YSOgJwEgEFwDnLRPslYwasyXBkKyNrFOefAElGQAFwTor29yna3ztoXV7xJOUUTEhCRwD+EgEFwDmp73hQ4c5jg9Y5RnuUmZ2bhI4A/CUCCoBz0vGjh9QV2DdonS0jU3bOPwGSjoAC4JxjjFGkLzzoIR6na5zGnH9ZcpoCEIeAAuCcYyL96u0e/PBOpnO0Rnl8SegIwBcRUACcc/p7utTxp12D1tkzs5SVnZeEjgB8EQEFwDkn0ndC3e3/M2CNzZ6pvPMuTVJHAL4ooYBSV1enmTNnKi8vT4WFhbr++uvV3NwcV9PT06OamhqNHTtWubm5qq6uVltbW1xNS0uL5s+fr5ycHBUWFuree+9Vf//gD+sCgDNljFG0/3SevZMpd8mUJHQE4FQSCij19fWqqanRO++8o82bN6uvr09z5sxRd3d3rObuu+/Wq6++qhdffFH19fU6fPiwbrjhhtjySCSi+fPnq7e3V2+//baef/55rVmzRg888MDwbRUAfBVjFPr4w0HLbBmZGl1QmoSGAJyKzZhBbqM4gCNHjqiwsFD19fW68sorFQwGNW7cOK1du1Y33nijJOmjjz7SpZdeqoaGBs2aNUsbN27UNddco8OHD8vr9UqSVq9erRUrVujIkSNyOByDfm4oFJLb7VYwGJTL5Rpq+wDOQdH+PjX+e82gdZmjcvX1Rf+ShI6Ac0ci399ndA5KMBiUJOXn50uSGhsb1dfXp8rKyljNpEmTVFpaqoaGBklSQ0ODpk6dGgsnklRVVaVQKKS9e/ee8nPC4bBCoVDcBABnk4fLi4GUGnJAiUajuuuuu3TFFVdoypTPj9MGAgE5HA55PJ64Wq/Xq0AgEKv5y3BycvnJZadSV1cnt9sdm0pKSobaNoBzXLhr8MuLJclXPucsdwJgIEMOKDU1NdqzZ4/WrVs3nP2cUm1trYLBYGw6dOjQWf9MACNTV+vgd4+VPr/FPYDUyRzKm5YtW6YNGzZo+/btGj9+fGy+z+dTb2+vOjo64vaitLW1yefzxWrefffduPWdvMrnZM0XOZ1OOZ3OobQKAHE+afzNoDWZo3Ilnl4MpFRCe1CMMVq2bJnWr1+vrVu3auLEiXHLp0+frqysLG3ZsiU2r7m5WS0tLfL7/ZIkv9+v3bt3q729PVazefNmuVwulZWVncm2AMCAopE+6TSuCyiecZ3sGVlJ6AjAV0loD0pNTY3Wrl2rV155RXl5ebFzRtxut7Kzs+V2u7V48WItX75c+fn5crlcuvPOO+X3+zVr1ixJ0pw5c1RWVqZbb71Vjz32mAKBgO6//37V1NSwlwTAWdXT0S4TjQxal5NfzB4UIMUSCiirVq2SJF111VVx85977jnddtttkqQnnnhCdrtd1dXVCofDqqqq0rPPPhurzcjI0IYNG7R06VL5/X6NHj1aixYt0sMPP3xmWwIAg2jf+6b6w8cHrbNnOmQjoAApdUb3QUkV7oMCIFHGGO3b+LSCh059O4OT8i+cqdIrblJWNv+2AMMtafdBAYB0EY30KXo6h3fGlijDkZOEjgAMhIAC4JwQDrarv6dr0LrMUaNlzxjSBY4AhhEBBcA5oavtgHo7jw5Y48grkCOvIEkdARgIAQXAiGeMUV93hyK9Jwasyx5TpOwxRUnqCsBACCgARrxIX4/6Tgz+DK+sHLeyctxJ6AjAYAgoAEa8vq7PdPzYJwPW2OwZyhyVy+XFgEUQUACMeD2hdnW3/c+ANVk5HnkmlCepIwCDIaAAGNGMiSra1ztoXYYjW9ljipPQEYDTQUABMKKZSL96goFB62wZGcpwZCehIwCng4ACYETrOxHS4T++NmhdRtYozj8BLISAAmDEMsZ8/vRiEx2wzp7pUIn/fyepKwCng4ACYETrPR4cvMhmU7bHd/abAXDaCCgARrSu1v2nVWfj9vaApRBQAIxcxiiw63eDlhV/45okNAMgEQQUACOWiUbUHz4+aJ2nlPufAFZDQAEwYg1299iTsnLyuIIHsBgCCoAR68hHvx/0Cp5c74Wy2Tn/BLAaAgqAEavzk+ZBa/Ivulz2zKwkdAMgEQQUACNSpLdn0L0nkpQ9pkiy8U8hYDX8VQIYkToDBxTpCw9YY89yyp6ZxfkngAURUACMSJ9+9Hv193QOWOOZME3OvHFJ6ghAIggoAM5Zozw+ZThzUt0GgFMgoAAYcfp6OhXp6xm0LiNrlOzcQRawJAIKgBEnHPpUfYM8g2fUmCKNLjw/OQ0BSBgBBcCI03m4WScGuUmbY/QYOV2cfwJYFQEFwIhiohFFI/2D1mU6RysrOy8JHQEYCgIKgBEl0ntCvZ1HBy+02WTj/ieAZfHXCWBEOdER0Gd/ahqwJjM7T2Mvujw5DQEYEgIKgBHDGKNof68i4e4B6zIdOcor+lqSugIwFAQUACOHMeo/0TV4nc2uDMeos98PgCEjoAAYMaL9YR3dv2OQKptyfRcmpR8AQ5dQQKmrq9PMmTOVl5enwsJCXX/99Wpujn9a6FVXXSWbzRY33XHHHXE1LS0tmj9/vnJyclRYWKh7771X/f2Dn3UPAAOJ9vcq2LJ7wBqb3S7vlO8kqSMAQ5XQLRTr6+tVU1OjmTNnqr+/Xz/+8Y81Z84cffDBBxo9enSs7vbbb9fDDz8ce52T8/9vJR2JRDR//nz5fD69/fbbam1t1cKFC5WVlaVHHnlkGDYJAAZi0yiPL9VNABhEQgFl06ZNca/XrFmjwsJCNTY26sorr4zNz8nJkc936n8Afve73+mDDz7QG2+8Ia/Xq8suu0w//elPtWLFCj300ENyOBxD2AwA5zpjjLo/bRm0LsMxSjZ7RhI6AnAmzugclGDw81tJ5+fnx81/4YUXVFBQoClTpqi2tlbHjx+PLWtoaNDUqVPl9Xpj86qqqhQKhbR3795Tfk44HFYoFIqbAOCLPju4c9Ca82Zef/YbAXDGhvyUrGg0qrvuuktXXHGFpkyZEpv/ve99TxMmTFBxcbF27dqlFStWqLm5WS+99JIkKRAIxIUTSbHXgUDglJ9VV1enlStXDrVVAOeIo/sGO0FWyi2cmIROAJypIQeUmpoa7dmzR2+99Vbc/CVLlsR+njp1qoqKijR79mwdOHBAF144tDPna2trtXz58tjrUCikkpKSoTUOYEQa7N4nJ2Vm58pms53lbgCcqSEd4lm2bJk2bNigN998U+PHjx+wtqKiQpK0f/9+SZLP51NbW1tczcnXX3XeitPplMvlipsA4C91H/mzZKID1oy5YLrsWdz/BEgHCQUUY4yWLVum9evXa+vWrZo4cfBdpU1NTZKkoqIiSZLf79fu3bvV3t4eq9m8ebNcLpfKysoSaQcAYj5+9yWZaGTAmjzfxbJnZCWpIwBnIqFDPDU1NVq7dq1eeeUV5eXlxc4Zcbvdys7O1oEDB7R27VpdffXVGjt2rHbt2qW7775bV155pcrLyyVJc+bMUVlZmW699VY99thjCgQCuv/++1VTUyOn0zn8WwhgxDMmKhM1g9Y53YWyZwz5yDaAJEpoD8qqVasUDAZ11VVXqaioKDb98pe/lCQ5HA698cYbmjNnjiZNmqR77rlH1dXVevXVV2PryMjI0IYNG5SRkSG/36+/+7u/08KFC+PumwIAieg7HpKJ9KW6DQDDKKH/Shgz8P9QSkpKVF9fP+h6JkyYoNdeey2RjwaAr3Tko7fU23VswJq88y7VKLd3wBoA1sGzeACkvd7OTxUdZA9KbuH5cublD1gDwDoIKADSmolGZKIDX70jSbaMLMnGP3lAuuCvFUBa6z0eVN+Jge8uneEcLWduPvc/AdIIAQVAWjv+aYtOHPtkwJqc/PPkLpmcpI4ADAcCCoC01tv9mfqOBwessTtGKTM7L0kdARgOBBQAaSsa6VckfHzQOrs9UzbOPwHSCn+xANJWf0+nutr/NGBN5qg8eafNSU5DAIYNAQVA2urtOqbgn/9rwBp7ZpZyxhQlqSMAw4WAAiAtGWNO6/Ji2WzKcGSf/YYADCsCCoD0ZIx6gm2Dlo2b9K0kNANguBFQAKQlE42oK7B/kCqbPBOmJaUfAMOLgAIgLUUjffq0+e2Bi2xS9hhfchoCMKwIKADS0ulcXpxTMCEJnQA4GwgoANJSV/vBQWvGTbqC5+8AaYq/XABp6XDjq4PW5HovTEInAM4GAgqAtGNMdNDb20uf3wOFBwQC6YmAAiDt9HZ9JmPMgDWOvALZ7JlJ6gjAcCOgAEg7nza/LRPpG7DGO+U7yspxJ6kjAMONgAIg7QQ/3isTjQxY4xjtls2ekaSOAAw39n8CSJr+/v4zX4kxn08DlhhFokaRyMAh5qtkZGRw7gqQYgQUAElhjJHX61UoFDqj9ZR63ar7P9/RxCLPV9a8ufNPuvbHs3Wss2dIn7Fjxw594xvfGGKHAIYDAQVA0vT395/xXpRLxo+RJ9c5YE3bZ106GuxWJDrwnpavMtgJuADOPgIKgLQy+fxCjcnLVntviTr7xyoqu3LsIRU6/qws++cnzvb1R4ccTgBYAwEFQNoY5chQtjNTH3bNUqB3osLRHBnZlGUL6+PwJM10vaa2Y0Ht3Nea6lYBnCGu4gGQNsa6chV0flt/7pmsnmiejDIk2dVnsnWsr0hvd/yt2jt6tPdPR1LdKoAzREABkDYKxl+h/NL5/y+YfJFNnZF8vfXpd9XRNbSTYwFYBwEFQNrIyLAPcvmvTZx5AowMBBQAacGZlSHvmNGpbgNAkhBQAKQF75hcXf/NSaluA0CSEFAApAVHVobKvQGVjtorKfql5cYYdYYC+uXaHyW/OQDDLqGAsmrVKpWXl8vlcsnlcsnv92vjxo2x5T09PaqpqdHYsWOVm5ur6upqtbW1xa2jpaVF8+fPV05OjgoLC3XvvfcOz+2vAYxo4d5+Hfj4iDwnNmp0325lmG7ZFJVklGkLKy/zmP777fsUDnenulUAwyCh+6CMHz9ejz76qC6++GIZY/T888/ruuuu086dOzV58mTdfffd+u1vf6sXX3xRbrdby5Yt0w033KA//OEPkqRIJKL58+fL5/Pp7bffVmtrqxYuXKisrCw98sgjZ2UDAYwMh46EdPvPfqPzfR5NLPqj3N4KZY6eoAJ3ns4f26/LvJ/o6f0fp7pNAMPEZs7wns75+fl6/PHHdeONN2rcuHFau3atbrzxRknSRx99pEsvvVQNDQ2aNWuWNm7cqGuuuUaHDx+W1+uVJK1evVorVqzQkSNH5HA4TuszQ6GQ3G63brvtttN+D4DUe+6559TX1zds68vNdqjQk6NxntEqHDNar72zb1juIHvDDTeooKBgGDoE8Jd6e3u1Zs0aBYNBuVyuAWuHfCfZSCSiF198Ud3d3fL7/WpsbFRfX58qKytjNZMmTVJpaWksoDQ0NGjq1KmxcCJJVVVVWrp0qfbu3auvf/3rp/yscDiscDgce33yYWO33nqrcnNzh7oJAJLshRdeGNaA0nWiV10nevU/rR3Dtk5Juu666zRpEifkAsOtq6tLa9asOa3ahAPK7t275ff71dPTo9zcXK1fv15lZWVqamqSw+GQx+OJq/d6vQoEApKkQCAQF05OLj+57KvU1dVp5cqVX5o/Y8aMQRMYAGswxigj41Q3WLOeyZMna/r06aluAxhxEnmaecJX8VxyySVqamrSjh07tHTpUi1atEgffPBBoqtJSG1trYLBYGw6dOjQWf08AACQWgnvQXE4HLroooskSdOnT9d7772np556SjfddJN6e3vV0dERtxelra1NPp9PkuTz+fTuu+/Gre/kVT4na07F6XTK6Rz48eoAAGDkOOP7oESjUYXDYU2fPl1ZWVnasmVLbFlzc7NaWlrk9/slSX6/X7t371Z7e3usZvPmzXK5XCorKzvTVgAAwAiR0B6U2tpazZs3T6Wlpers7NTatWu1bds2vf7663K73Vq8eLGWL1+u/Px8uVwu3XnnnfL7/Zo1a5Ykac6cOSorK9Ott96qxx57TIFAQPfff79qamrYQwIAAGISCijt7e1auHChWltb5Xa7VV5ertdff13f/e53JUlPPPGE7Ha7qqurFQ6HVVVVpWeffTb2/oyMDG3YsEFLly6V3+/X6NGjtWjRIj388MPDu1UAACCtnfF9UFLh5H1QTuc6agDWYIyRx+NJ6Cz+VHn//fe5igc4CxL5/uZZPAAAwHIIKAAAwHIIKAAAwHIIKAAAwHKG/CweAEjUNddco+PHj6e6jUF98ZEdAJKPgAIgKWw2m1544YVUtwEgTXCIBwAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWA4BBQAAWE5CAWXVqlUqLy+Xy+WSy+WS3+/Xxo0bY8uvuuoq2Wy2uOmOO+6IW0dLS4vmz5+vnJwcFRYW6t5771V/f//wbA0AABgRMhMpHj9+vB599FFdfPHFMsbo+eef13XXXaedO3dq8uTJkqTbb79dDz/8cOw9OTk5sZ8jkYjmz58vn8+nt99+W62trVq4cKGysrL0yCOPDNMmAQCAdGczxpgzWUF+fr4ef/xxLV68WFdddZUuu+wyPfnkk6es3bhxo6655hodPnxYXq9XkrR69WqtWLFCR44ckcPhOK3PDIVCcrvdCgaDcrlcZ9I+AABIkkS+v4d8DkokEtG6devU3d0tv98fm//CCy+ooKBAU6ZMUW1trY4fPx5b1tDQoKlTp8bCiSRVVVUpFApp7969X/lZ4XBYoVAobgIAACNXQod4JGn37t3y+/3q6elRbm6u1q9fr7KyMknS9773PU2YMEHFxcXatWuXVqxYoebmZr300kuSpEAgEBdOJMVeBwKBr/zMuro6rVy5MtFWAQBAmko4oFxyySVqampSMBjUr3/9ay1atEj19fUqKyvTkiVLYnVTp05VUVGRZs+erQMHDujCCy8ccpO1tbVavnx57HUoFFJJScmQ1wcAAKwt4UM8DodDF110kaZPn666ujpNmzZNTz311ClrKyoqJEn79++XJPl8PrW1tcXVnHzt8/m+8jOdTmfsyqGTEwAAGLnO+D4o0WhU4XD4lMuampokSUVFRZIkv9+v3bt3q729PVazefNmuVyu2GEiAACAhA7x1NbWat68eSotLVVnZ6fWrl2rbdu26fXXX9eBAwe0du1aXX311Ro7dqx27dqlu+++W1deeaXKy8slSXPmzFFZWZluvfVWPfbYYwoEArr//vtVU1Mjp9N5VjYQAACkn4QCSnt7uxYuXKjW1la53W6Vl5fr9ddf13e/+10dOnRIb7zxhp588kl1d3erpKRE1dXVuv/++2Pvz8jI0IYNG7R06VL5/X6NHj1aixYtirtvCgAAwBnfByUVuA8KAADpJyn3QQEAADhbCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByCCgAAMByMlPdwFAYYyRJoVAoxZ0AAIDTdfJ7++T3+EDSMqB0dnZKkkpKSlLcCQAASFRnZ6fcbveANTZzOjHGYqLRqJqbm1VWVqZDhw7J5XKluqW0FQqFVFJSwjgOA8Zy+DCWw4NxHD6M5fAwxqizs1PFxcWy2wc+yyQt96DY7Xadd955kiSXy8UvyzBgHIcPYzl8GMvhwTgOH8byzA225+QkTpIFAACWQ0ABAACWk7YBxel06sEHH5TT6Ux1K2mNcRw+jOXwYSyHB+M4fBjL5EvLk2QBAMDIlrZ7UAAAwMhFQAEAAJZDQAEAAJZDQAEAAJaTlgHlmWee0fnnn69Ro0apoqJC7777bqpbspzt27fr2muvVXFxsWw2m15++eW45cYYPfDAAyoqKlJ2drYqKyu1b9++uJpjx45pwYIFcrlc8ng8Wrx4sbq6upK4FalXV1enmTNnKi8vT4WFhbr++uvV3NwcV9PT06OamhqNHTtWubm5qq6uVltbW1xNS0uL5s+fr5ycHBUWFuree+9Vf39/MjclpVatWqXy8vLYTa78fr82btwYW84YDt2jjz4qm82mu+66KzaP8Tw9Dz30kGw2W9w0adKk2HLGMcVMmlm3bp1xOBzmP/7jP8zevXvN7bffbjwej2lra0t1a5by2muvmX/8x380L730kpFk1q9fH7f80UcfNW6327z88svmv/7rv8zf/M3fmIkTJ5oTJ07EaubOnWumTZtm3nnnHfP73//eXHTRReaWW25J8pakVlVVlXnuuefMnj17TFNTk7n66qtNaWmp6erqitXccccdpqSkxGzZssW8//77ZtasWeav/uqvYsv7+/vNlClTTGVlpdm5c6d57bXXTEFBgamtrU3FJqXEb37zG/Pb3/7W/Pd//7dpbm42P/7xj01WVpbZs2ePMYYxHKp3333XnH/++aa8vNz84Ac/iM1nPE/Pgw8+aCZPnmxaW1tj05EjR2LLGcfUSruAcvnll5uamprY60gkYoqLi01dXV0Ku7K2LwaUaDRqfD6fefzxx2PzOjo6jNPpNL/4xS+MMcZ88MEHRpJ57733YjUbN240NpvNfPLJJ0nr3Wra29uNJFNfX2+M+XzcsrKyzIsvvhir+fDDD40k09DQYIz5PCza7XYTCARiNatWrTIul8uEw+HkboCFjBkzxvzbv/0bYzhEnZ2d5uKLLzabN282f/3Xfx0LKIzn6XvwwQfNtGnTTrmMcUy9tDrE09vbq8bGRlVWVsbm2e12VVZWqqGhIYWdpZeDBw8qEAjEjaPb7VZFRUVsHBsaGuTxeDRjxoxYTWVlpex2u3bs2JH0nq0iGAxKkvLz8yVJjY2N6uvrixvLSZMmqbS0NG4sp06dKq/XG6upqqpSKBTS3r17k9i9NUQiEa1bt07d3d3y+/2M4RDV1NRo/vz5ceMm8TuZqH379qm4uFgXXHCBFixYoJaWFkmMoxWk1cMCP/30U0UikbhfBknyer366KOPUtRV+gkEApJ0ynE8uSwQCKiwsDBueWZmpvLz82M155poNKq77rpLV1xxhaZMmSLp83FyOBzyeDxxtV8cy1ON9cll54rdu3fL7/erp6dHubm5Wr9+vcrKytTU1MQYJmjdunX64x//qPfee+9Ly/idPH0VFRVas2aNLrnkErW2tmrlypX61re+pT179jCOFpBWAQVIpZqaGu3Zs0dvvfVWqltJS5dccomampoUDAb161//WosWLVJ9fX2q20o7hw4d0g9+8ANt3rxZo0aNSnU7aW3evHmxn8vLy1VRUaEJEyboV7/6lbKzs1PYGaQ0u4qnoKBAGRkZXzqLuq2tTT6fL0VdpZ+TYzXQOPp8PrW3t8ct7+/v17Fjx87JsV62bJk2bNigN998U+PHj4/N9/l86u3tVUdHR1z9F8fyVGN9ctm5wuFw6KKLLtL06dNVV1enadOm6amnnmIME9TY2Kj29nZ94xvfUGZmpjIzM1VfX6+nn35amZmZ8nq9jOcQeTwefe1rX9P+/fv5vbSAtAooDodD06dP15YtW2LzotGotmzZIr/fn8LO0svEiRPl8/nixjEUCmnHjh2xcfT7/ero6FBjY2OsZuvWrYpGo6qoqEh6z6lijNGyZcu0fv16bd26VRMnToxbPn36dGVlZcWNZXNzs1paWuLGcvfu3XGBb/PmzXK5XCorK0vOhlhQNBpVOBxmDBM0e/Zs7d69W01NTbFpxowZWrBgQexnxnNourq6dODAARUVFfF7aQWpPks3UevWrTNOp9OsWbPGfPDBB2bJkiXG4/HEnUWNz8/w37lzp9m5c6eRZP7lX/7F7Ny50/z5z382xnx+mbHH4zGvvPKK2bVrl7nuuutOeZnx17/+dbNjxw7z1ltvmYsvvvicu8x46dKlxu12m23btsVdinj8+PFYzR133GFKS0vN1q1bzfvvv2/8fr/x+/2x5ScvRZwzZ45pamoymzZtMuPGjTunLkW87777TH19vTl48KDZtWuXue+++4zNZjO/+93vjDGM4Zn6y6t4jGE8T9c999xjtm3bZg4ePGj+8Ic/mMrKSlNQUGDa29uNMYxjqqVdQDHGmJ///OemtLTUOBwOc/nll5t33nkn1S1ZzptvvmkkfWlatGiRMebzS41/8pOfGK/Xa5xOp5k9e7Zpbm6OW8fRo0fNLbfcYnJzc43L5TLf//73TWdnZwq2JnVONYaSzHPPPRerOXHihPmHf/gHM2bMGJOTk2P+9m//1rS2tsat509/+pOZN2+eyc7ONgUFBeaee+4xfX19Sd6a1Pn7v/97M2HCBONwOMy4cePM7NmzY+HEGMbwTH0xoDCep+emm24yRUVFxuFwmPPOO8/cdNNNZv/+/bHljGNq2YwxJjX7bgAAAE4trc5BAQAA5wYCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsBwCCgAAsJz/C7ke9PBywjwQAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "# **Exercise 1:**\n",
        "\n",
        "Can you design a dynamic programming based policy for the agent as in assignment 1? If so, design it and demonstrate that it solves the cart pole problem.\n",
        "\n",
        "--\n",
        "\n",
        "**Reasons DP is Not Suitable for CartPole:**\n",
        "\n",
        "- CartPole has a continuous state space (cart position, cart velocity, pole angle, pole angular velocity), which DP typically cannot handle without discretization. Discretizing such a space would lead to an impractically large number of states.\n",
        "\n",
        "- Discretizing a four-dimensional continuous space into a finite set of states would result in a massive state space, making computations infeasible and inefficient.\n",
        "\n",
        "- While CartPole has a discrete action space (move left or right), combining it with a large, discretized state space exacerbates the computational challenges of DP.\n",
        "\n",
        "- DP requires a complete model of the environment, including transition dynamics and rewards, to compute optimal value functions or policies. In reinforcement learning problems like CartPole, this model is typically not available or accessible.\n",
        "\n",
        "- DP methods assume offline computation with full knowledge of the environment dynamics and state space. In contrast, CartPole and similar RL problems require online learning where agents interact with the environment to learn without complete knowledge of its dynamics.\n",
        "\n",
        "Dynamic programming, due to its reliance on a discrete and known state space, is not suitable for solving CartPole directly. This is because dynamic programming requires an explicit model of the environment's transition dynamics, which describes how the state changes in response to actions. In the CartPole problem, the environment is continuous and often complex, making it difficult to define a precise model for these dynamics.\n"
      ],
      "metadata": {
        "id": "XGErf3hRtdZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 2:**\n",
        "\n",
        "Can you design a Monte Carlo based policy for the agent? What ingredients do you require? Explain the design flow, and execute it. Show that it works, or indicate why you can't proceed.\n",
        "\n",
        "--\n",
        "\n",
        "### **Monte Carlo-based Policy for CartPole Environment**\n",
        "\n",
        "**What ingredients do you require?:**\n",
        "\n",
        "- **Environment:** The CartPole environment from OpenAI Gym.\n",
        "- **Policy:** policy specifying the probability distribution over actions given a state.\n",
        "- **Sample Generation:** Generating episodes by following the policy and interacting with the environment.\n",
        "- **Monte Carlo Estimation:** Estimating the value function using sampled episodes.\n",
        "- **Policy Improvement:** Updating the policy based on the estimated value function.\n",
        "\n",
        "--\n",
        "\n",
        "**Explain the design flow, and execute it**\n",
        "\n",
        "\n",
        "**Initialization:**\n",
        "\n",
        "- The CartPole environment is initialized using Gym (gym.make('CartPole-v0')).\n",
        "- A stochastic policy (epsilon_soft_policy) is defined, which is initially Œµ-soft (exploratory).\n",
        "- The number of episodes (num_episodes) for policy evaluation is set.\n",
        "- Data structures (Q and N) are initialized to store the action-value function (Q) and visitation counts.\n",
        "\n",
        "**Episode Generation:**\n",
        "\n",
        "- For each episode, the environment is reset (env.reset()) to get the initial state.\n",
        "- Actions are chosen based on the Œµ-soft policy (epsilon_soft_policy) until the episode ends (done=True).\n",
        "- States (episode_states), actions (episode_actions), and rewards (episode_rewards) are recorded during the episode.\n",
        "\n",
        "**Monte Carlo Estimation (First-Visit MC):**\n",
        "\n",
        "- After an episode ends, the return $G_t$ is calculated for each state-action pair from the first occurrence to the end of the episode.\n",
        "- For each state-action pair, the Q-value is updated using the Monte Carlo incremental update rule: $ùëÑ(ùëÜ_ùë°,ùê¥_ùë°)‚ÜêùëÑ(ùëÜ_ùë°,ùê¥_ùë°)+ùõº(ùê∫_ùë°‚àíùëÑ(ùëÜ_ùë°,ùê¥_ùë°))$\n",
        "\n",
        "where $ùõº$ is the learning rate.\n",
        "Policy Improvement:\n",
        "\n",
        "- The policy is updated to be greedy with respect to the updated Q-values: $œÄ(s)=argmax_aQ(s,a)$\n",
        "\n",
        "**Repeat:**\n",
        "\n",
        "- Steps 2 to 4 are repeated for a number of episodes (num_episodes) until the policy converges or a sufficient number of episodes are completed."
      ],
      "metadata": {
        "id": "sYWW4mr0w6Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define Œµ-soft policy parameters\n",
        "epsilon = 0.1  # Exploration parameter\n",
        "action_space_size = env.action_space.n  # Number of actions\n",
        "\n",
        "# Initialize Q-values and visitation counts\n",
        "Q = defaultdict(lambda: np.zeros(action_space_size))  # Q-function: defaultdict with initial values of zeros\n",
        "N = defaultdict(lambda: np.zeros(action_space_size))  # Visit count: defaultdict with initial values of zeros\n",
        "\n",
        "# Function to generate an Œµ-soft policy\n",
        "def epsilon_soft_policy(state):\n",
        "    state_tuple = tuple(state)  # Convert state to tuple for hashing\n",
        "    probabilities = np.ones(action_space_size) * epsilon / action_space_size  # Initialize equal probabilities for each action\n",
        "    best_action = np.argmax(Q[state_tuple])  # Get the best action based on Q-values\n",
        "    probabilities[best_action] += 1.0 - epsilon  # Update the probability of the best action\n",
        "    return np.random.choice(action_space_size, p=probabilities)  # Choose action based on probabilities\n",
        "\n",
        "# Function to run on-policy first-visit MC control\n",
        "def on_policy_first_visit_MC(env, num_episodes, gamma):\n",
        "    total_iterations = 0  # Initialize total iterations counter\n",
        "    for episode in range(num_episodes):\n",
        "        episode_states = []  # List to store states in the current episode\n",
        "        episode_actions = []  # List to store actions in the current episode\n",
        "        episode_rewards = []  # List to store rewards in the current episode\n",
        "        state = env.reset()  # Reset environment to start new episode and get initial state\n",
        "        while True:\n",
        "            action = epsilon_soft_policy(state)  # Choose action based on Œµ-soft policy\n",
        "            next_state, reward, done, _ = env.step(action)  # Take action and observe next state and reward\n",
        "            episode_states.append(state)  # Append current state to episode states\n",
        "            episode_actions.append(action)  # Append chosen action to episode actions\n",
        "            episode_rewards.append(reward)  # Append received reward to episode rewards\n",
        "            state = next_state  # Update current state to next state\n",
        "            if done:  # If episode is done (pole has fallen or max steps reached), exit loop\n",
        "                break\n",
        "\n",
        "        G = 0  # Initialize return (cumulative reward)\n",
        "        # Loop backward through episode to calculate returns and update Q-values\n",
        "        for t in range(len(episode_states) - 1, -1, -1):\n",
        "            state = tuple(episode_states[t])  # Convert current state to tuple for hashing\n",
        "            action = episode_actions[t]  # Get action at current time step\n",
        "            reward = episode_rewards[t]  # Get reward at current time step\n",
        "            G = gamma * G + reward  # Update return (cumulative reward)\n",
        "            N[state][action] += 1  # Increment visitation count for state-action pair\n",
        "            # Update Q-value using incremental update rule based on the return and visitation count\n",
        "            Q[state][action] += (G - Q[state][action]) / N[state][action]\n",
        "            pi_S_t = np.argmax(Q[state])  # Greedy policy improvement: Get the best action for the current state\n",
        "            if action != pi_S_t:  # Check if action is not the best action\n",
        "                break  # Exit loop if action is not optimal\n",
        "\n",
        "        # Print Q-values after every 100 episodes\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode: {episode + 1}\")\n",
        "            for idx, (state, q_values) in enumerate(Q.items()):\n",
        "                if idx < 3:  # Print Q-values for the first three states for brevity\n",
        "                    print(\"Print Q-values for the first three states for brevity\")\n",
        "                    print(f\"State: {state} Q-values: {q_values}\")\n",
        "            print()\n",
        "\n",
        "        total_iterations += len(episode_states)  # Accumulate total iterations\n",
        "\n",
        "# Run on-policy first-visit MC control\n",
        "num_episodes = 1000  # Number of episodes\n",
        "gamma = 1.0  # Discount factor\n",
        "\n",
        "on_policy_first_visit_MC(env, num_episodes, gamma)  # Execute on-policy first-visit MC control\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YLkxbumaByrx",
        "outputId": "fdb8d7e5-5a74-4ab6-a7e9-0f1d7a26ba42"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 200\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 300\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 400\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 500\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 600\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 700\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 800\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 900\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n",
            "Episode: 1000\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.04755219, -0.0043567624, -0.049008306, 0.04283846) Q-values: [12.  0.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.047465056, -0.19874294, -0.048151534, 0.31966496) Q-values: [ 0. 11.]\n",
            "Print Q-values for the first three states for brevity\n",
            "State: (0.043490198, -0.0029694792, -0.041758236, 0.012194062) Q-values: [10.  0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38Wmt6a-KZ1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}