{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **APS1080: Introduction to Reinforcement Learning**\n",
        "---\n",
        "**Student Name**:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Student ID:**\n",
        "\n",
        "---\n",
        "**Assignment 2:** Monte Carlo\n",
        "\n",
        "------"
      ],
      "metadata": {
        "id": "PqdoKLzmoaoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercises**\n",
        "--------\n",
        "**1. Explain clearly why V_pi is not useful in the MC development above?**\n",
        "\n",
        "\n",
        "In the context of Monte Carlo methods for reinforcement learning, $ð‘‰ðœ‹$ represents the value function under a given policy\n",
        "$ðœ‹$. Specifically, $ð‘‰ðœ‹(ð‘ )$ is the expected return (total discounted reward) starting from state $ð‘ $ and following policy $ðœ‹$ thereafter. In the case of MC, $ð‘‰ðœ‹(ð‘ )$ is not useful for the following reasons:\n",
        "\n",
        "- The MC methods discussed are designed for situations where we do not have a model of the environment (transition dynamics $ð‘$). Without a model, we cannot use analytical methods to derive $ð‘‰_ðœ‹$.\n",
        "- To improve the policy from $ðœ‹_ð‘˜$ to $ðœ‹_{ð‘˜+1}$, we need to know which action $ð‘Ž$ in state $ð‘ $ maximizes the expected return. This requires the action-value function $ð‘„_{ðœ‹ð‘˜}(ð‘ ,ð‘Ž)$, not just $ð‘‰_{ðœ‹ð‘˜}(ð‘ )$.\n",
        "-Monte Carlo methods are based on sampling and averaging returns to learn value functions. Since we lack the transition dynamics, the focus is on learning through experience. To effectively learn and improve the policy, the agent must explore the entire state-action space $ð‘†Ã—ð´$. This requires a stochastic policy where every action $ð‘Ž$ has a non-zero probability of being chosen in every state $ð‘ $. This ensures sufficient coverage of all state-action pairs, which is critical for accurate estimation of $ð‘„_k$.\n",
        "- The requirement for effective learning is that the agent experiences the full state-action space. This means the episodes must cover all possible state-action pairs sufficiently. $ð‘‰_ðœ‹$ does not provide direct information about action values, making it less useful for ensuring thorough exploration.\n",
        "$ð‘„_ðœ‹(ð‘ ,ð‘Ž)$, on the other hand, directly informs the agent about the value of specific actions in specific states, guiding exploration and policy improvement.\n",
        "- MC methods involve updating value estimates based on empirical returns from sampled episodes. These updates are naturally formulated in terms of state-action pairs ($ð‘„_ðœ‹(ð‘ ,ð‘Ž)$) rather than just states.\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "\n",
        "**2. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q_pi_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case:**\n",
        "\n",
        "--\n",
        "\n",
        "**a. Will we obtain Q_pi_k from eval?**\n",
        "\n",
        "With a truncated evaluation in the modified MC algorithm, we will obtain an approximation of $ð‘„_{ðœ‹ð‘˜}$ rather than the exact $Q_{Ï€k}$.\n",
        "\n",
        "--\n",
        "\n",
        "**b. If not why are we able to truncate Eval? Explain clearly.**\n",
        "\n",
        "We are able to truncate the evaluation because the goal is to iteratively improve the policy rather than to precisely compute\n",
        "$ð‘„_{ðœ‹k}$. Truncating evaluation still provides useful estimates of action values, allowing for practical and gradual policy improvement. This aligns with the principle of Generalized Policy Iteration (GPI), where approximate evaluations suffice for driving the policy towards optimality over multiple iterations.\n",
        "\n",
        "--\n",
        "\n",
        "**c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval_trunc, is it possible to converge on a sub-optimal policy pi_c? Is this a stable fixed point of the GPI for MC? Explain clearly.**\n",
        "\n",
        "It is possible to converge on a sub-optimal policy using truncated evaluation, even with Exploring Starts (ES). This sub-optimal policy can be a stable fixed point of the GPI process because truncated evaluations might introduce approximation errors, and the iterative policy improvement step could stabilize on these sub-optimal approximations. Consequently, the policy may stop improving and converge to a sub-optimal policy.\n",
        "\n",
        "--\n",
        "\n",
        "**3. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead).**\n",
        "\n",
        "To synthesize a stochastic policy using Monte Carlo methods with Exploring Starts and truncated evaluation, start by initializing a random policy where each action has a non-zero probability of being chosen in each state, and initialize action-value estimates arbitrarily. For each episode, begin from a randomly chosen state-action pair to ensure thorough exploration. Follow the current policy to generate episodes, and for each state-action pair in the episode, calculate and update action-value estimates using truncated returns. Improve the policy by making it epsilon-greedy with respect to the updated action-value estimates, meaning with high probability, choose the best-known action, and with a small probability, choose a random action to encourage exploration. Repeat this process of generating episodes, updating action values, and refining the policy until it stabilizes or a set number of iterations is reached. This iterative approach enables the development of a stochastic policy that effectively balances exploration and exploitation based on sampled experiences.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "H-4kJ7zjpzOT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDvssQd64Pf"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5esgX013vPe"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install xvfb\n"
      ],
      "metadata": {
        "id": "M4auH7yOHEcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbi2xaFo31Sj"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGqXqJxoAsHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6737bc98-7734-4f3c-c52f-c7ce70288f7b"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7dce64c49ea0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L4YayzR4FYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "a917ae2a-2fda-44c9-b445-43628ace02b8"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations that were run: 28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Print state and action space sizes\n",
        "state_space_size = env.observation_space.shape[0]  # Get the size of the state space\n",
        "action_space_size = env.action_space.n  # Get the size of the action space\n",
        "print(\"State space size:\", state_space_size)\n",
        "print(\"Action space size:\", action_space_size)\n",
        "\n",
        "# Define a function to round the state values\n",
        "def round_state(state):\n",
        "    return tuple(round(x, 1) for x in state)\n",
        "\n",
        "# Define a function to generate an episode\n",
        "def generate_episode(env):\n",
        "    episode = []  # Initialize an empty list to store the episode\n",
        "    state = env.reset()  # Reset the environment to start a new episode\n",
        "    state = round_state(state)  # Round the state values\n",
        "    for _ in range(50000):\n",
        "        action = np.random.randint(action_space_size)  # Choose a random action\n",
        "        next_state, reward, done, _ = env.step(action)  # Take a step in the environment\n",
        "        next_state = round_state(next_state)  # Round the next state values\n",
        "        episode.append((state, action, reward))  # Append the state, action, and reward to the episode\n",
        "        state = next_state  # Update the current state\n",
        "        if done:  # If the episode is done\n",
        "            break  # Exit the loop\n",
        "    return episode  # Return the generated episode\n",
        "\n",
        "# Off-Policy Monte Carlo Control algorithm\n",
        "def off_MC_control(env, max_episodes, gamma):\n",
        "    Q = defaultdict(lambda: np.zeros(action_space_size))  # Initialize Q-values\n",
        "    C = defaultdict(lambda: np.zeros(action_space_size))  # Initialize visitation counts\n",
        "\n",
        "    for episode_num in range(max_episodes + 1):  # Iterate over episodes\n",
        "        episode = generate_episode(env)  # Generate an episode\n",
        "        G, W = 0, 1  # Initialize return and importance sampling ratio\n",
        "\n",
        "        for t in range(len(episode) - 1, -1, -1):  # Loop backwards through the episode\n",
        "            state, action, reward = episode[t]  # Get state, action, and reward\n",
        "            G = gamma * G + reward  # Update return\n",
        "            C[state][action] += W  # Update visitation counts\n",
        "            Q[state][action] += (G - Q[state][action]) * (W / C[state][action])  # Update Q-values\n",
        "            pi_S_t = np.argmax(Q[state])  # Greedy policy\n",
        "            if action != pi_S_t:  # Check if action is not optimal\n",
        "                break  # Exit the loop\n",
        "            W *= 1. / (1.0 / action_space_size)  # Update importance sampling ratio\n",
        "\n",
        "\n",
        "        # Calculate and print average iterations\n",
        "        total_iterations = sum([len(episode) for episode in episode])\n",
        "        avg_iterations = total_iterations / (episode_num + 1)\n",
        "        print(\"Episode:\", episode_num, \"Average iterations:\", avg_iterations)\n",
        "\n",
        "    return Q  # Return the learned Q-values\n",
        "\n",
        "# Run Off-Policy Monte Carlo Control\n",
        "Q = off_MC_control(env, 10, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsN3gOnT0tHd",
        "outputId": "168bc9fc-73b8-48c2-d2a4-0cca2c3b372e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State space size: 4\n",
            "Action space size: 2\n",
            "Episode: 0 Average iterations: 75.0\n",
            "Episode: 1 Average iterations: 18.0\n",
            "Episode: 2 Average iterations: 17.0\n",
            "Episode: 3 Average iterations: 11.25\n",
            "Episode: 4 Average iterations: 13.8\n",
            "Episode: 5 Average iterations: 16.0\n",
            "Episode: 6 Average iterations: 6.0\n",
            "Episode: 7 Average iterations: 10.125\n",
            "Episode: 8 Average iterations: 11.333333333333334\n",
            "Episode: 9 Average iterations: 4.8\n",
            "Episode: 10 Average iterations: 3.272727272727273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4gK4wAbRTCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}