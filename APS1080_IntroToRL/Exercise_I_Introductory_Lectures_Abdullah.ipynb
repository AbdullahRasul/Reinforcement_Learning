{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Course**: APS1080: Introduction to Reinforcement Learning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Student Name:**\n",
        "\n",
        "**Student ID:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Exercise:** I: Introductory Lectures\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vNwkiQOiF2Xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may be started before or after Lecture II.\n",
        "\n",
        "Per the syllabus: exercises are marked for completion but not correctness. Exercises are an opportunity for you to test your knowledge and ask questions if something is unclear."
      ],
      "metadata": {
        "id": "8iwvbVC9Gh7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A. Read Chapter 3 and do the following exercises:**\n",
        "\n",
        "# 3.7-3.9, 3.12, 3.18, 3.19\n",
        "\n",
        "(Note: the nomenclature used in part a of Lecture II was based on the prior edition of the textbook; I will refresh the nomenclature and symbols of Lecture II-b and going forward, so everything will be consistent. At this point, it should be fairly easy to see the differences: the new edition of the book uses capitals differently, and changes the symbol for the Return to be Gt versus Rt, etc. Please using the new nomenclature in your work).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lT9fNub2HduY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.7**\n",
        "\n",
        "Imagine that you are designing a robot to run a maze. You decide to give it a\n",
        "reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
        "\n",
        "\n",
        "## **Solution (3.7):**\n",
        "\n",
        "*Reasons for no improvement:*\n",
        "\n",
        "The issue lies in the reward structure you have set up for the learning agent. By only giving a reward of +1 for escaping the maze and a reward of zero at all other times, you are providing very sparse and infrequent feedback. This can make it difficult for the agent to learn which actions lead to escaping the maze because the positive reward is only received at the end of an episode and provides no intermediate guidance.\n",
        "\n",
        "*My Explanation:*\n",
        "\n",
        "In reinforcement learning, the agent needs to explore and learn from the consequences of its actions to improve its performance over time. When rewards are sparse, the agent has little to no indication of whether its actions are moving it closer to or further from the goal until it actually reaches the goal. This can result in very slow learning or even no improvement at all, as the agent may not be able to reliably associate the correct actions with the reward due to the long delay between taking the action and receiving the reward.\n",
        "\n",
        "*Better ways to communicate with the agent of what we want to achieve:*\n",
        "\n",
        "\n",
        "\n",
        "*   Use a reward structure based on the distance to the exit. We could give a reward based on the reduction in distance to the exit with each step. This encourages the agent to take actions that bring it closer to the goal.\n",
        "\n",
        "*   Penalize the agent for taking too long to escape the maze. You could introduce a small negative reward for each step taken, incentivizing the agent to find the shortest path to the exit.\n",
        "\n",
        "* We could give small positive rewards for actions that move the agent closer to the exit and small negative rewards for actions that move it further away.\n",
        "\n",
        "* By providing more informative and frequent feedback through these methods, we can help the agent to better understand and achieve the goal of escaping the maze\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "oXKlDXQtOIZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.8**\n",
        "\n",
        "Suppose γ = 0.5 and the following sequence of rewards is received R1 = -1,\n",
        "R2 = 2, R3 = 6, R4 = 3, and R5 = 2, with T = 5. What are G0 , G1 , . . . , G5?\n",
        "\n",
        "Hint: Work backwards.\n",
        "\n",
        "## **Solution (3.8)**\n",
        "\n",
        "Formula (Equation 3.9 in the textbook):\n",
        "\n",
        "\\begin{align*}\n",
        "G_{t}=R_{t+1}+γG_{t+1}\\\\\n",
        "\\end{align*}\n",
        "**Given**:\n",
        "\n",
        "γ=0.5\n",
        "\n",
        "Sequence:\n",
        "$R_1 =  1,R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2$\n",
        "\n",
        "T = 5\n",
        "\n",
        "**Steps**:\n",
        "\n",
        "1. $G_5 = 0$ (since there are no rewards after the episode ends.)\n",
        "2. $G_4 = R_5 = 2$\n",
        "3. $G_3 = R_4 + γG_4 = 3 + 0.5 * 2 = 4$  \n",
        "4. $G_2 = R_3 + γG_3 = 6 + 0.5 * 4 = 8$\n",
        "5. $G_1 = R_2 + γG_2 = 2 + 0.5 * 8 = 6$\n",
        "6. $G_0 = R_1 + γG_1 = -1 + 0.5 *6 = 2$\n",
        "\n",
        "$G_5 = 0$\n",
        "\n",
        "$G_4 = 2$\n",
        "\n",
        "$G_3 = 4$\n",
        "\n",
        "$G_2 = 8$\n",
        "\n",
        "$G_1 = 6$\n",
        "\n",
        "$G_0 = 2$    \n",
        "\n",
        "--------------\n"
      ],
      "metadata": {
        "id": "nnb04aO8OIXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.9**\n",
        "\n",
        "Suppose γ = 0.9 and the reward sequence is R 1 = 2 followed by an infinite\n",
        "sequence of 7s. What are $G_1$ and $G_0$ ?\n",
        "\n",
        "\n",
        "## **Solution (3.9)**\n",
        "\n",
        "Formulas:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t =\\sum_{k=0}^\\infty \\gamma^k  = 1 / 1-γ\\\\\n",
        "G_{t}=R_{t+1}+γG_{t+1}\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Given:\n",
        "\n",
        "γ=0.9\n",
        "\n",
        "Sequence: $R_1=2, R_2,R_3, ....... = 7$\n",
        "\n",
        "\n",
        "**To calculate $G_1$:**\n",
        "\n",
        "\\begin{align*}\n",
        "G_1 &\\doteq \\sum_{k=0}^\\infty \\gamma^k R_{1+k+1} \\\\\n",
        "&= \\sum_{k=0}^\\infty 0.9^k \\cdot 7 \\\\\n",
        "&= \\frac{7}{1 - 0.9} \\\\\n",
        "&= \\frac{7}{0.1} \\\\\n",
        "&= 70\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "**To calculate** $G_0$:\n",
        "\n",
        "\\begin{align*}\n",
        "G_0 &= R_1 + \\gamma G_1 \\\\\n",
        "&= 2 + 0.9 \\times 70 \\\\\n",
        "&= 2 + 63 \\\\\n",
        "&= 65\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, the values are:\n",
        "$$G_1 = 70,\n",
        "G_0 = 65$$\n",
        "\n",
        "----------------------"
      ],
      "metadata": {
        "id": "VxOuIjVgOIUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.12**\n",
        "\n",
        "Give an equation for $v_π$ in terms of $q_π$ and π.\n",
        "\n",
        "\n",
        "## **Solution (3.12)**\n",
        "\n",
        "### Definitions:\n",
        "1. State Value Function $$v_\\pi(s):\n",
        "   v_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]$$\n",
        "   This is the expected return starting from state s and following policy π\n",
        "\n",
        "2. Action Value Function $$q_\\pi(s, a):\n",
        "   q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]$$\n",
        "   This is the expected return starting from state \\(s\\), taking action \\(a\\), and then following policy π.\n",
        "  \n",
        "### Derivation:\n",
        "To derive the relationship between $v_\\pi(s)$ and $q_\\pi(s, a)$, we start with the definition of $v_\\pi(s)$:\n",
        "\n",
        "$$\n",
        "v_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]$$\n",
        "\n",
        "Since $G_t$ is the return, it can be defined as the sum of the reward $R_{t+1}$ and the discounted return from the next state $S_{t+1}$:\n",
        "\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma G_{t+1}\n",
        "$$\n",
        "\n",
        "Thus, the value function can be expressed as:\n",
        "$$\n",
        "v_\\pi(s) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\right]$$\n",
        "\n",
        "\n",
        "Using the law of total expectation, we condition on the first action \\(A_t\\) and the next state \\(S_{t+1}\\):\n",
        "\n",
        "$$\n",
        "v_\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a \\right]\n",
        "$$\n",
        "\n",
        "Next, we use the fact that the reward $R_{t+1}$ and the next state $S_{t+1}$ depend only on the current state \\(s\\) and action \\(a\\), and that the future returns depend on the next state $S_{t+1}$:\n",
        "\n",
        "$$\n",
        "v_\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} \\mathbb{P}(S_{t+1} = s' \\mid S_t = s, A_t = a) \\left[ R(s, a, s') + \\gamma \\mathbb{E}_\\pi \\left[ G_{t+1} \\mid S_{t+1} = s' \\right] \\right]$$\n",
        "\n",
        "Recognize that the inner expectation $$\\mathbb{E}_\\pi \\left[ G_{t+1} \\mid S_{t+1} = s' \\right]$$ is just the value function $v_\\pi(s')$:\n",
        "\n",
        "$$\n",
        "v_\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} \\mathbb{P}(S_{t+1} = s' \\mid S_t = s, A_t = a) \\left[ R(s, a, s') + \\gamma v_\\pi(s') \\right]\n",
        "$$\n",
        "\n",
        "The action value function $q_\\pi(s, a)$ is defined as:\n",
        "\n",
        "$$\n",
        "q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]$$\n",
        "\n",
        "Thus, we can express it similarly:\n",
        "\n",
        "$$\n",
        "q_\\pi(s, a) = \\sum_{s'} \\mathbb{P}(S_{t+1} = s' \\mid S_t = s, A_t = a) \\left[ R(s, a, s') + \\gamma v_\\pi(s') \\right]\n",
        "$$\n",
        "\n",
        "Therefore, the state value function can be written in terms of the action value function:\n",
        "\n",
        "$$\n",
        "v_\\pi(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a \\mid s) q_\\pi(s, a)\n",
        "$$\n",
        "\n",
        "\n",
        "This equation shows how the value of a state under a policy $\\pi$ is the expected value of the action-value function, weighted by the policy's probability of taking each action in that state.\n",
        "\n",
        "--------\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hvuzFhwFOISK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.18**\n",
        "\n",
        "The value of a state depends on the values of the actions possible in that\n",
        "state and on how likely each action is to be taken under the current policy. We can\n",
        "think of this in terms of a small backup diagram rooted at the state and considering each\n",
        "possible action. Give the equation corresponding to this intuition and diagram for the value at the root\n",
        "node, v ⇡ ( s ), in terms of the value at the expected leaf node, q ⇡ ( s, a ), given S t = s . This\n",
        "equation should include an expectation conditioned on following the policy, ⇡ . Then give\n",
        "a second equation in which the expected value is written out explicitly in terms of ⇡ ( a|s )\n",
        "such that no expected value notation appears in the equation.\n",
        "\n",
        "\n",
        "## **Solution (3.18)**\n",
        "\n",
        "Equations corresponding to the question.\n",
        "\n",
        "$$ \\begin{align*}\n",
        "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}\n",
        "\\left[ q_{\\pi}(s,a) | S_t=s \\right]\n",
        "\\\\\n",
        "&= \\sum_{a\\in \\mathcal{A}} \\left[ \\pi(a|s) q_{\\pi} (s,a) \\right]\n",
        "\\end{align*} $$\n",
        "\n",
        "----------------------------------------------\n"
      ],
      "metadata": {
        "id": "RLtJrdnxOIPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.19**\n",
        "The value of an action, q ⇡ ( s, a ), depends on the expected next reward and\n",
        "the expected sum of the remaining rewards. Again we can think of this in terms of a\n",
        "small backup diagram, this one rooted at an action (state–action pair) and branching to\n",
        "the possible next states:\n",
        "Give the equation corresponding to this intuition and diagram for the action value,\n",
        "q ⇡ (s, a), in terms of the expected next reward, R t+1 , and the expected next state value,\n",
        "v ⇡ ( S t+1 ), given that S t = s and A t = a . This equation should include an expectation but\n",
        "not one conditioned on following the policy. Then give a second equation, writing out the\n",
        "expected value explicitly in terms of p ( s 0 , r |s, a ) defined by (3.2) , such that no expected\n",
        "value notation appears in the equation.\n",
        "\n",
        "## **Solution 3.19**\n",
        "\n",
        "The equation corresponding to the intuition and diagram for the action value, in terms of the expected next reward and the expected next state value , given that $S_t = s$ and $A_t =a $, can be represented as follows:\n",
        "\n",
        "$$ \\begin{align*}\n",
        "q_{\\pi}(s,a) &= \\mathbb{E}_{p(s',r|s,a)}\\left[\n",
        "r+v_{\\pi}(s')\n",
        "| S_t=s,A_t=a\n",
        "\\right]\n",
        "\\end{align*} $$\n",
        "\n",
        "This equation expresses $q_{\\pi}(s, a)$ as the expected sum of the immediate reward $R_{t+1}$ and the discounted expected value of the next state $S_{t+1}$ under policy $\\pi$, given that the current state is s and the action taken is a.\n",
        "\n",
        "The second equation, written explicitly in terms of $p(s', r \\mid s, a)$, can be obtained by replacing the expectation notation with the explicit expression:\n",
        "\n",
        "$$ \\begin{align*}\n",
        "\\\\\n",
        "&= \\sum_{s'\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}\\left[\n",
        "    p(s',r|s,a)(r+v_{\\pi}(s'))\n",
        "\\right]\n",
        "\\end{align*} $$\n",
        "\n",
        "------------------------------\n",
        "----------------------------"
      ],
      "metadata": {
        "id": "louIqJWhOIMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **B. Practical**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-LYu0nbBKVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\n",
        "\n",
        "Links to an external site.\n",
        "\n",
        "contains solutions to the programmatic work in the textbook (second edition).\n",
        "\n",
        "Download the full repository to your local machine. Navigate to the solution to Chapter 1's tic tac toe example.\n",
        "\n",
        "Load this in colab and try it out. If the training epoch takes too long, reduce the number of iterations.\n",
        "\n",
        "----------------------"
      ],
      "metadata": {
        "id": "RwTJJsODTpXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Anatomy**\n",
        "\n",
        "\n",
        "Read the code and observe how the concepts of the course thus far relate to it. Comment on these correspondences. What does an RL agent solve? What should a RL agent have inside of it? Do you observe these in the program?\n",
        "\n",
        "\n",
        "## **Answer**\n",
        "\n",
        "### Anatomy of the Code\n",
        "\n",
        "#### Reinforcement Learning Concepts in the Code\n",
        "\n",
        "The code provided is an implementation of a Reinforcement Learning (RL) agent that plays tic-tac-toe.\n",
        "\n",
        "#### 1. **Environment (State Class)**\n",
        "The `State` class represents the tic-tac-toe board. It includes methods to initialize the board, check if the game has ended, compute the hash value of the state, and get the next state.\n",
        "\n",
        "   - **RL Concepts**:\n",
        "     - **State Space**: The board configurations represent different states in the environment.\n",
        "     - **State Transition Function**: The `next_state` method defines how the state transitions when a move is made.\n",
        "\n",
        "#### 2. **Agent (Player Class)**\n",
        "The `Player` class represents an RL agent. It includes methods for choosing actions (`act`), updating value estimates (`backup`), and saving/loading policies.\n",
        "\n",
        "   - **RL Concepts**:\n",
        "     - **Policy**: The agent's strategy for choosing actions is encapsulated in the `act` method. It can choose actions based on an epsilon-greedy policy.\n",
        "     - **Value Function**: The agent maintains value estimates (`estimations`) for different states, which are updated using Temporal Difference (TD) learning in the `backup` method.\n",
        "\n",
        "#### 3. **Learning (Training and Backup)**\n",
        "The `train` function trains two agents (player1 and player2) by playing multiple games and updating their value estimates after each game.\n",
        "   - **RL Concept**:\n",
        "     - **TD Learning**: The `backup` method implements TD learning, updating value estimates based on the difference between the predicted value and the observed value.\n",
        "     - **Episodes**: Each game of tic-tac-toe represents an episode. The `train` function runs multiple episodes to train the agents.\n",
        "\n",
        "#### 4. **Judger Class**\n",
        "\n",
        "The `Judger` class manages the game, alternating between two players and determining the winner.\n",
        "   - **RL Concept**:\n",
        "     - **Interaction with Environment**: The `Judger` class facilitates the interaction between the agents and the environment, allowing them to take actions and observe the results.\n",
        "\n",
        "#### 5. **HumanPlayer Class**\n",
        "The `HumanPlayer` class allows a human to play against the RL agent.\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "### **What Does an RL Agent Solve?**\n",
        "\n",
        "An RL agent solves decision-making problems where an agent learns to take actions in an environment to maximize cumulative rewards. In this case, the agent learns to play tic-tac-toe optimally.\n",
        "\n",
        "### **What Should an RL Agent Have Inside of It?**\n",
        "\n",
        "An RL agent should have the following components:\n",
        "* **Policy**: A strategy for choosing actions based on the current state.\n",
        "* **Value Function**: An estimation of the expected return (future rewards) for states or state-action pairs.\n",
        "* **Learning Algorithm**: A method to update the value function based on new experiences\n",
        "* **Exploration Mechanism**: A way to explore different actions to discover * potentially better strategies\n",
        "\n",
        "### **Do you observe these in the program?**\n",
        "\n",
        "- **Policy**: Implemented in the `act` method of the `Player` class using an epsilon-greedy approach.\n",
        "- **Value Function**: The `estimations` dictionary in the `Player` class stores the value estimates for different states.\n",
        "- **Learning Algorithm**: The `backup` method in the `Player` class uses TD learning to update value estimates.\n",
        "- **Exploration Mechanism**: The `epsilon` parameter in the `Player` class controls the probability of choosing a random action (exploration).\n",
        "\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "RHRRakrmVNhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Instrumentation and Play**\n",
        "\n",
        "Modify the code so it trains against you interactively. Instrument the code to print the value function as it learns, and also when it takes an exploit vs explore action. Observe how the agent takes exploitative actions and explorative actions, and how the value function changes.\n",
        "\n",
        "Let the agent train against you for N games. Comment on how the agent's competence increases as N increases.\n",
        "\n",
        "## **Answer**\n"
      ],
      "metadata": {
        "id": "7oQqzEpkYbdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3\n",
        "BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self):\n",
        "        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.winner = None\n",
        "        self.hash_val = None\n",
        "        self.end = None\n",
        "\n",
        "    def hash(self):\n",
        "        if self.hash_val is None:\n",
        "            self.hash_val = 0\n",
        "            for i in np.nditer(self.data):\n",
        "                self.hash_val = self.hash_val * 3 + i + 1\n",
        "        return self.hash_val\n",
        "\n",
        "    def is_end(self):\n",
        "        if self.end is not None:\n",
        "            return self.end\n",
        "        results = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            results.append(np.sum(self.data[i, :]))\n",
        "        for i in range(BOARD_COLS):\n",
        "            results.append(np.sum(self.data[:, i]))\n",
        "        trace = 0\n",
        "        reverse_trace = 0\n",
        "        for i in range(BOARD_ROWS):\n",
        "            trace += self.data[i, i]\n",
        "            reverse_trace += self.data[i, BOARD_ROWS - 1 - i]\n",
        "        results.append(trace)\n",
        "        results.append(reverse_trace)\n",
        "        for result in results:\n",
        "            if result == 3:\n",
        "                self.winner = 1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "            if result == -3:\n",
        "                self.winner = -1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "        sum_values = np.sum(np.abs(self.data))\n",
        "        if sum_values == BOARD_SIZE:\n",
        "            self.winner = 0\n",
        "            self.end = True\n",
        "            return self.end\n",
        "        self.end = False\n",
        "        return self.end\n",
        "\n",
        "    def next_state(self, i, j, symbol):\n",
        "        new_state = State()\n",
        "        new_state.data = np.copy(self.data)\n",
        "        new_state.data[i, j] = symbol\n",
        "        return new_state\n",
        "\n",
        "    def print_state(self):\n",
        "        for i in range(BOARD_ROWS):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.data[i, j] == 1:\n",
        "                    token = '*'\n",
        "                elif self.data[i, j] == -1:\n",
        "                    token = 'x'\n",
        "                else:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')\n",
        "\n",
        "\n",
        "def get_all_states_impl(current_state, current_symbol, all_states):\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if current_state.data[i][j] == 0:\n",
        "                new_state = current_state.next_state(i, j, current_symbol)\n",
        "                new_hash = new_state.hash()\n",
        "                if new_hash not in all_states:\n",
        "                    is_end = new_state.is_end()\n",
        "                    all_states[new_hash] = (new_state, is_end)\n",
        "                    if not is_end:\n",
        "                        get_all_states_impl(new_state, -current_symbol, all_states)\n",
        "\n",
        "\n",
        "def get_all_states():\n",
        "    current_symbol = 1\n",
        "    current_state = State()\n",
        "    all_states = dict()\n",
        "    all_states[current_state.hash()] = (current_state, current_state.is_end())\n",
        "    get_all_states_impl(current_state, current_symbol, all_states)\n",
        "    return all_states\n",
        "\n",
        "\n",
        "all_states = get_all_states()\n",
        "\n",
        "\n",
        "class Judger:\n",
        "    def __init__(self, player1, player2):\n",
        "        self.p1 = player1\n",
        "        self.p2 = player2\n",
        "        self.current_player = None\n",
        "        self.p1_symbol = 1\n",
        "        self.p2_symbol = -1\n",
        "        self.p1.set_symbol(self.p1_symbol)\n",
        "        self.p2.set_symbol(self.p2_symbol)\n",
        "        self.current_state = State()\n",
        "\n",
        "    def reset(self):\n",
        "        self.p1.reset()\n",
        "        self.p2.reset()\n",
        "\n",
        "    def alternate(self):\n",
        "        while True:\n",
        "            yield self.p1\n",
        "            yield self.p2\n",
        "\n",
        "    def play(self, print_state=False):\n",
        "        alternator = self.alternate()\n",
        "        self.reset()\n",
        "        current_state = State()\n",
        "        self.p1.set_state(current_state)\n",
        "        self.p2.set_state(current_state)\n",
        "        if print_state:\n",
        "            current_state.print_state()\n",
        "        while True:\n",
        "            player = next(alternator)\n",
        "            i, j, symbol = player.act()\n",
        "            next_state_hash = current_state.next_state(i, j, symbol).hash()\n",
        "            current_state, is_end = all_states[next_state_hash]\n",
        "            self.p1.set_state(current_state)\n",
        "            self.p2.set_state(current_state)\n",
        "            if print_state:\n",
        "                current_state.print_state()\n",
        "            if is_end:\n",
        "                return current_state.winner\n",
        "\n",
        "\n",
        "class Player:\n",
        "    def __init__(self, step_size=0.1, epsilon=0.1):\n",
        "        self.estimations = dict()\n",
        "        self.step_size = step_size\n",
        "        self.epsilon = epsilon\n",
        "        self.states = []\n",
        "        self.greedy = []\n",
        "        self.symbol = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.greedy = []\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.states.append(state)\n",
        "        self.greedy.append(True)\n",
        "\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        for hash_val in all_states:\n",
        "            state, is_end = all_states[hash_val]\n",
        "            if is_end:\n",
        "                if state.winner == self.symbol:\n",
        "                    self.estimations[hash_val] = 1.0\n",
        "                elif state.winner == 0:\n",
        "                    self.estimations[hash_val] = 0.5\n",
        "                else:\n",
        "                    self.estimations[hash_val] = 0\n",
        "            else:\n",
        "                self.estimations[hash_val] = 0.5\n",
        "\n",
        "    def backup(self):\n",
        "        states = [state.hash() for state in self.states]\n",
        "        print(\"Value Function Update:\")\n",
        "        for i in reversed(range(len(states) - 1)):\n",
        "            state = states[i]\n",
        "            td_error = self.greedy[i] * (self.estimations[states[i + 1]] - self.estimations[state])\n",
        "            self.estimations[state] += self.step_size * td_error\n",
        "            print(f\"State: {state}, Value: {self.estimations[state]}\")\n",
        "\n",
        "    def act(self):\n",
        "        state = self.states[-1]\n",
        "        next_states = []\n",
        "        next_positions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if state.data[i, j] == 0:\n",
        "                    next_positions.append([i, j])\n",
        "                    next_states.append(state.next_state(i, j, self.symbol).hash())\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = next_positions[np.random.randint(len(next_positions))]\n",
        "            action.append(self.symbol)\n",
        "            self.greedy[-1] = False\n",
        "            print(f\"Exploring: {action}\")\n",
        "            return action\n",
        "        values = []\n",
        "        for hash_val, pos in zip(next_states, next_positions):\n",
        "            values.append((self.estimations[hash_val], pos))\n",
        "        np.random.shuffle(values)\n",
        "        values.sort(key=lambda x: x[0], reverse=True)\n",
        "        action = values[0][1]\n",
        "        action.append(self.symbol)\n",
        "        print(f\"Exploiting: {action}\")\n",
        "        return action\n",
        "\n",
        "    def save_policy(self):\n",
        "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
        "            pickle.dump(self.estimations, f)\n",
        "\n",
        "    def load_policy(self):\n",
        "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
        "            self.estimations = pickle.load(f)\n",
        "\n",
        "\n",
        "class HumanPlayer:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.symbol = None\n",
        "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.state = state\n",
        "\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "\n",
        "    def act(self):\n",
        "        self.state.print_state()\n",
        "        key = input(\"Input your position:\")\n",
        "        data = self.keys.index(key)\n",
        "        i = data // BOARD_COLS\n",
        "        j = data % BOARD_COLS\n",
        "        return i, j, self.symbol\n",
        "\n",
        "\n",
        "def train_against_human(epochs):\n",
        "    player = Player(epsilon=0.1)\n",
        "    human = HumanPlayer()\n",
        "    judger = Judger(player, human)\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "        winner = judger.play(print_state=True)\n",
        "        if winner == player.symbol:\n",
        "            print(\"Agent wins!\")\n",
        "        elif winner == human.symbol:\n",
        "            print(\"Human wins!\")\n",
        "        else:\n",
        "            print(\"It's a tie!\")\n",
        "        player.backup()\n",
        "        judger.reset()\n",
        "    player.save_policy()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    epochs = int(input(\"Enter number of games to train against human: \"))\n",
        "    train_against_human(epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFjIA6xITneY",
        "outputId": "2df832d7-dda8-4ae9-e26c-4009c90f6e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of games to train against human: 7\n",
            "\n",
            "Epoch 1\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 0, 1]\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:q\n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [0, 2, 1]\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 1, 1]\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "Input your position:c\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | * | x | \n",
            "-------------\n",
            "Human wins!\n",
            "Value Function Update:\n",
            "State: 3940.0, Value: 0.45\n",
            "State: 3937.0, Value: 0.495\n",
            "State: 4018.0, Value: 0.4995\n",
            "State: 3289.0, Value: 0.49995\n",
            "State: 9850.0, Value: 0.499995\n",
            "State: 9841.0, Value: 0.4999995\n",
            "\n",
            "Epoch 2\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploring: [0, 0, 1]\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 0, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 2, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | x | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Exploiting: [2, 1, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | x | 0 | \n",
            "-------------\n",
            "| * | * | * | \n",
            "-------------\n",
            "Agent wins!\n",
            "Value Function Update:\n",
            "State: 15359.0, Value: 0.55\n",
            "State: 15440.0, Value: 0.505\n",
            "State: 15439.0, Value: 0.5005\n",
            "State: 15682.0, Value: 0.50005\n",
            "State: 15673.0, Value: 0.500005\n",
            "State: 16402.0, Value: 0.5000005\n",
            "State: 9841.0, Value: 0.4999995\n",
            "\n",
            "Epoch 3\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [0, 0, 1]\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 0, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 2, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:x\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Exploiting: [1, 1, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Agent wins!\n",
            "Value Function Update:\n",
            "State: 15437.0, Value: 0.55\n",
            "State: 15440.0, Value: 0.5095000000000001\n",
            "State: 15439.0, Value: 0.5014\n",
            "State: 15682.0, Value: 0.500185\n",
            "State: 15673.0, Value: 0.500023\n",
            "State: 16402.0, Value: 0.5000027499999999\n",
            "State: 9841.0, Value: 0.49999982499999995\n",
            "\n",
            "Epoch 4\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [0, 0, 1]\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:z\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [1, 0, 1]\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 2, 1]\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "Human wins!\n",
            "Value Function Update:\n",
            "State: 15908.0, Value: 0.45\n",
            "State: 15907.0, Value: 0.495\n",
            "State: 16636.0, Value: 0.4995\n",
            "State: 16393.0, Value: 0.49995\n",
            "State: 16402.0, Value: 0.49999747499999997\n",
            "State: 9841.0, Value: 0.49999958999999994\n",
            "\n",
            "Epoch 5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 1, 1]\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 2, 1]\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | * | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | * | \n",
            "-------------\n",
            "Input your position:z\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "Exploiting: [1, 0, 1]\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "Human wins!\n",
            "Value Function Update:\n",
            "State: 9350.0, Value: 0.45\n",
            "State: 9107.0, Value: 0.495\n",
            "State: 9116.0, Value: 0.4995\n",
            "State: 9115.0, Value: 0.49995\n",
            "State: 9844.0, Value: 0.499995\n",
            "State: 9841.0, Value: 0.49999913099999993\n",
            "\n",
            "Epoch 6\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [1, 1, 1]\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [1, 2, 1]\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:c\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Exploiting: [0, 0, 1]\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:z\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "Exploiting: [0, 1, 1]\n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "Input your position:x\n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| x | x | x | \n",
            "-------------\n",
            "Human wins!\n",
            "Value Function Update:\n",
            "State: 18444.0, Value: 0.45\n",
            "State: 16257.0, Value: 0.495\n",
            "State: 16266.0, Value: 0.4995\n",
            "State: 9705.0, Value: 0.49995\n",
            "State: 9706.0, Value: 0.499995\n",
            "State: 9679.0, Value: 0.4999995\n",
            "State: 9922.0, Value: 0.49999994999999997\n",
            "State: 9841.0, Value: 0.49999921289999993\n",
            "\n",
            "Epoch 7\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [1, 0, 1]\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Exploiting: [2, 2, 1]\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "Exploiting: [2, 0, 1]\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:q\n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Exploiting: [2, 1, 1]\n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "| * | * | * | \n",
            "-------------\n",
            "Agent wins!\n",
            "Value Function Update:\n",
            "State: 2723.0, Value: 0.55\n",
            "State: 9284.0, Value: 0.505\n",
            "State: 9275.0, Value: 0.5005\n",
            "State: 9356.0, Value: 0.50005\n",
            "State: 9355.0, Value: 0.500005\n",
            "State: 10084.0, Value: 0.5000005\n",
            "State: 9841.0, Value: 0.4999993416099999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis of the Results:**\n",
        "\n",
        "- Each game results in updates to the agent's value function, reflecting how it adjusts its estimates of the state's value based on the outcome of the game.\n",
        "Initially, many state values start around 0.5 (indicating uncertainty) and adjust based on the game's outcome.\n",
        "\n",
        "\n",
        "- The agent won 3 out of the 7 games.\n",
        "In each win, the value function updates show increases in the estimated values for the states encountered, indicating that the agent is learning which states are beneficial.\n",
        "\n",
        "\n",
        "- When the agent loses, the values of the states it visited are decreased, which helps it avoid those states or actions in future games.\n",
        "When the agent wins, the values of the states are increased, reinforcing the actions that led to a positive outcome.\n",
        "Observations on Competence Increase:\n",
        "Improvement Over Games:\n",
        "\n",
        "- The agent’s wins show an upward adjustment in the value function, indicating that it is learning to recognize and exploit beneficial states.\n",
        "The decrease in state values after losses also shows the agent is learning to avoid suboptimal actions.\n",
        "Balance of Exploration and Exploitation:\n",
        "\n",
        "- The agent is making a mix of exploratory and exploitative moves. Early in training, it might lean more towards exploration due to its high epsilon value (0.1).\n",
        "As training progresses, even with exploration, the agent starts to favor exploitation more due to the value function becoming more accurate.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F3BZuKiGRlJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Appendix: Lecture Notes**\n",
        "\n",
        "## **Lecture 1 (May 11)**\n",
        "\n",
        "**Comp Science:**\n",
        "\n",
        "* Sort, Search, etc   --> Probability and Models\n",
        "\n",
        "* Extends to Neural Networks\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "**Control Theory:**\n",
        "\n",
        "* Model based: Closed loop Control Systems\n",
        "\n",
        "* No Model: Stchastic Control\n",
        "\n",
        "* ~ Models: Dynamic Programming (Extends to RL)\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "**ML Question:**\n",
        "\n",
        "* What to and how to learn? (Never passed this stage for Supervised and Unsupervised)\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "**RL phases:**\n",
        "\n",
        "* Part 1: Fundamentals (starts with Dynamic programming and leads to other fundamental methods)\n",
        "\n",
        "* Part 2: Adding Neural Networks (NN) to the fundamental techniques and to answer “what and how to learn” + DQN + PG + RLHF and more\n",
        "\n",
        "* Part 3: Hybridize computer science techniques with new learned RL techniques. (alpha zero, etc)\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "**RL is AI**\n",
        "\n",
        "* Machine -> Agent -> where AI lives\n",
        "\n",
        "* Environment -> Where problem is modelled\n",
        "\n",
        "* Machine takes actions -> A -> set of discrete actions\n",
        "\n",
        "* Feedback from environment -> S (state, sensory feedback) -> State space\n",
        "\n",
        "* Reward: Those aspects of S that relate to our Goal -> Map to real number\n",
        "\n",
        "* Don’t over design the reward signal\n",
        "\n",
        "* For example: Winning 100, stale mate 50 and loss –100\n",
        "\n",
        "* Machine works on how reward is designed\n",
        "\n",
        "-----\n",
        "* Machine and Environment operate in time.\n",
        "\n",
        "* Interaction of M and E in Time:\n",
        "\n",
        "* Autonomous systems are dynamic (operate in time)\n",
        "\n",
        "* Sequences of state, action and reward in time -> Episode, Traces, Playouts\n",
        "\n",
        "---\n",
        "\n",
        "* AI Problem (Action Selection Problem): At Time i, Given Si, what action (Ai) should M select from the possible set of actions.\n",
        "\n",
        "* RL Answer: Select the action such sequence of subsequent reward is maximize. (sum of rewards is maximized)\n",
        "\n",
        "* How to access future: That’s where the learning comes.\n",
        "\n",
        "* Long term and short-term memory can be handled using gamma factor.\n",
        "\n",
        "Basic RL illustration and Equations below:\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FoVYFkvtbth-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lecture 2 (May 18)**\n",
        "\n",
        "Lecture 2 (May 18)\n",
        "\n",
        "\n",
        "**AI Problem:**\n",
        "\n",
        "\n",
        "* Agent (Machine) interacts with the environment by taking actions\n",
        "\n",
        "* Environment responds with senses, states, feedback, etc with a characterized signal called reward (we define this signal)\n",
        "\n",
        "* We define the reward signal by looking at the environment.\n",
        "\n",
        "* Reward (Real number) = By looking at environment state and giving it a score.  \n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "Reward Examples:\n",
        "\n",
        "* Goal is to not lose then stale mate and winning will be positive numbers and loss will be negative\n",
        "\n",
        "* Goal is to win at all costs then stale mate and loss will both be negative numbers\n",
        "\n",
        "* Penalizes excess time taken.\n",
        "\n",
        "* R = How good or bad\n",
        "\n",
        "* R = Engineer defined\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "* AI Problem: At a given time, what action to be selec ted from a possible set of actions (Action Selection Problem)\n",
        "  \n",
        "\n",
        "* RL Answer: Select action that maximizes the future rewards.\n",
        "\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "Discount Factor\n",
        "\n",
        "* The discount factor is a constant to reflect the value of the reward signal over time in reinforcement learning  \n",
        "\n",
        "* Hyperparameter\n",
        "\n",
        "* How long term our solver to be\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "**Lecture 2 topic:**  \n",
        "\n",
        "* How to compute the means of action section when we have a model\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "* Mean of action selection problem = Policy of Machine (π)\n",
        "\n",
        "* Model of the environment (p)\n",
        "\n",
        "* Deterministic model = Control Theory\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "Policy (π):\n",
        "\n",
        "* Property of an agent.\n",
        "\n",
        "* It is how an agent (M) selects action.\n",
        "\n",
        "* Action selection Mechanism.\n",
        "\n",
        "* At best: Stochastic Model\n",
        "\n",
        "* Usually: No model\n",
        "\n",
        "* Π (A | S) = Distribution\n",
        "\n",
        "* It is a table (like a data structure) with N rows and K columns.\n",
        "\n",
        "* The numbers in the table must in real and in range [0,1] and numbers in each * row sum to 1 (probability distribution)\n",
        "\n",
        "* We can start an agent with a basic policy\n",
        "\n",
        "* Simple example below:\n",
        "\n",
        "_____________________________________________________\n",
        "\n",
        "Model:\n",
        "\n",
        "* Property of Environment\n",
        "\n",
        "* What can we do with p ?\n",
        "\n",
        "\n",
        "\n",
        "* Markovian Property: The future state depends only on the present state and action, and not on the sequence of states and actions that preceded it.\n",
        "\n",
        "* A structure of tree: Start with a state then possible actions with a probability of π then branching to possible states and rewards with probability of p. With this, we can find sequence of rewards.\n",
        "\n",
        "* A treelike structure would help in finding expected return of each state (value function)\n",
        "\n",
        "* Based on the value function results see the best state. Pick an action that maximizes the probability p of that state.\n",
        "\n",
        "* A value function is an important quantity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Z9mN46zh0cM"
      }
    }
  ]
}