{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Student Name**:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Student ID:**\n",
        "\n",
        "---\n",
        "**Assignment 1:** Dynamic Programming\n",
        "\n",
        "------\n"
      ],
      "metadata": {
        "id": "9fb2eQ9Y5JzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question:** What is the purpose of OpenAI gyms and how is it going to help us in our RL education?\n",
        "\n",
        "----\n",
        "\n",
        "## **Answer:**\n",
        "\n",
        "The OpenAI Gym is a toolkit (open source Python library) designed to provide a standardized environment for developing and testing reinforcement learning (RL) algorithms.\n",
        "\n",
        "- Provides a common interface for different RL environments, making it easier to compare and benchmark different algorithms.\n",
        "- Simplifies the process of creating and interacting with RL environments.\n",
        "- Includes a diverse set of environments ranging from simple text-based environments to complex simulations.\n",
        "\n",
        "--\n",
        "- Implement RL algorithms and immediately test them in a controlled environment.\n",
        "- experimentation with different algorithms and hyperparameters.\n",
        "- Includes implementations of standard RL algorithms.\n",
        "\n",
        "\n",
        "------\n"
      ],
      "metadata": {
        "id": "FEpsgxkm_4SO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iS40R9okStg"
      },
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVX1AjRWkueO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "60dbe433-e9e3-4e5c-d432-6a460b86bce5"
      },
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question:** what is the data in this structure saying? Relate this to the course presentation of P\n",
        "\n",
        "----\n",
        "\n",
        "## **Answer**:\n",
        "\n",
        "The provided structure for P in the FrozenLakeNotSlippery-v0 environment represents the transition dynamics of the MDP. (*book definition: The function p defines the dynamics of the MDP.*)\n",
        "\n",
        "- **State** (0 to 15): Represents the states in the environment.\n",
        "- **Action** (0 to 3): Represents the possible actions (0: Left, 1: Down, 2: Right, 3: Up).\n",
        "- **Values** (List of tuples): Each tuple in the list describes a possible outcome: Probability (1.0): Since the environment is deterministic, the transition probability is always 1.0.\n",
        "- **Next State:** The resulting state after taking the action.\n",
        "- **Reward**: The reward received after the transition (1 if next state is 15, otherwise 0).\n",
        "- **Is Terminal** (True/False): Indicates whether the episode ends after this transition (True if next state is terminal).\n",
        "\n",
        "--\n",
        "\n",
        "**Data Structure**\n",
        "\n",
        "**Dictionary Format:**\n",
        "\n",
        "*{(state, action): [(probability, next_state, reward, is_terminal)]}*\n",
        "\n",
        "This shows the possible transitions for each state-action pair.\n",
        "\n",
        "--\n",
        "\n",
        "**Examples** (for explanation)\n",
        "\n",
        "*State 0:*\n",
        "\n",
        "- **Action 0 (Left):** [(1.0, 0, 0.0, False)] - Stays in state 0.\n",
        "- **Action 1 (Down):** [(1.0, 4, 0.0, False)] - Moves to state 4.\n",
        "- **Action 2 (Right):** [(1.0, 1, 0.0, False)] - Moves to state 1.\n",
        "- **Action 3 (Up):** [(1.0, 0, 0.0, False)] - Stays in state 0.\n",
        "\n",
        "*State 14:*\n",
        "\n",
        "- **Action 2 (Right):** [(1.0, 15, 1.0, True)] - Moves to state 15 (the goal) with a reward of 1, ending the episode.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "M1lB-1Fj6VQK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyn_w3ulkyZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5c47722c-a6c2-4af7-ba75-c6dfe7cd4dc9"
      },
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zND5ArI8k_qQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "200a94e6-4315-4dd4-b665-f16d75a59bf8"
      },
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tp9YzRljnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "944377ba-0933-4c30-e0ea-08710757afc2"
      },
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFGNZNowluz2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7450882c-dc53-4f51-ee49-c65727927d32"
      },
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample from S: 7  ...  sample from A: 1\n",
            "sample from S: 15  ...  sample from A: 2\n",
            "sample from S: 13  ...  sample from A: 0\n",
            "sample from S: 0  ...  sample from A: 0\n",
            "sample from S: 11  ...  sample from A: 1\n",
            "sample from S: 10  ...  sample from A: 3\n",
            "sample from S: 15  ...  sample from A: 3\n",
            "sample from S: 1  ...  sample from A: 0\n",
            "sample from S: 9  ...  sample from A: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOQL5JxsmcEd"
      },
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLV6e43mmwx1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dc6548cc-5e3a-43bf-daf1-69798021189f"
      },
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "3\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "2\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 5\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "## **Question:** draw a table indicating the correspondence between the action you input (a number) and the logic action performed."
      ],
      "metadata": {
        "id": "c5PoGVDGGOA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question: draw a table indicating the correspondence between the action\n",
        "you input (a number) and the logic action performed.\n",
        "------------------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "import warnings\n",
        "\n",
        "# to hide warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "#for better tabular visualzation\n",
        "import pandas as pd\n",
        "\n",
        "actions= {\n",
        "    \"Action input\": [0, 1, 2, 3],\n",
        "    \"Logic action perfomed\": [\"Left\", \"Down\", \"Right\", \"Up\"]\n",
        "}\n",
        "\n",
        "actions_table = pd.DataFrame(actions)\n",
        "actions_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "r9gPYlqlCEK1",
        "outputId": "72d62c37-b6d7-4a65-f399-dcc0e3dc419a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Action input Logic action perfomed\n",
              "0             0                  Left\n",
              "1             1                  Down\n",
              "2             2                 Right\n",
              "3             3                    Up"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc9e3b08-5fa5-40da-b5da-c8de7621f0d9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Action input</th>\n",
              "      <th>Logic action perfomed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Up</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc9e3b08-5fa5-40da-b5da-c8de7621f0d9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc9e3b08-5fa5-40da-b5da-c8de7621f0d9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc9e3b08-5fa5-40da-b5da-c8de7621f0d9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5f1c2996-d455-4e7b-b218-5278fac08eec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f1c2996-d455-4e7b-b218-5278fac08eec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5f1c2996-d455-4e7b-b218-5278fac08eec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_fc1b0c42-6bff-459a-a023-0dd767f3176a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('actions_table')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fc1b0c42-6bff-459a-a023-0dd767f3176a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('actions_table');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "actions_table",
              "summary": "{\n  \"name\": \"actions_table\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Action input\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Logic action perfomed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Down\",\n          \"Up\",\n          \"Left\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Question**: draw a table that illustrates what the symbols on the render image mean?"
      ],
      "metadata": {
        "id": "w0lsRhRKGbLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "symbols_data = {'Symbols':['S', 'H', 'F', 'G'],\n",
        "        'Meaning':['Start', 'Hole', 'Frozen', 'Goal']}\n",
        "\n",
        "df = pd.DataFrame(symbols_data)\n",
        "df\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Qe9afoz8C1u4",
        "outputId": "b1c085d2-f14b-41fe-94c1-befeb80e9d41"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Symbols Meaning\n",
              "0       S   Start\n",
              "1       H    Hole\n",
              "2       F  Frozen\n",
              "3       G    Goal"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96c6c716-2ef9-4919-8f10-6696600745c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbols</th>\n",
              "      <th>Meaning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S</td>\n",
              "      <td>Start</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>H</td>\n",
              "      <td>Hole</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>Frozen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>G</td>\n",
              "      <td>Goal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96c6c716-2ef9-4919-8f10-6696600745c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96c6c716-2ef9-4919-8f10-6696600745c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96c6c716-2ef9-4919-8f10-6696600745c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-44470e9f-0ae5-4ac4-a39b-56ecddea2706\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44470e9f-0ae5-4ac4-a39b-56ecddea2706')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-44470e9f-0ae5-4ac4-a39b-56ecddea2706 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_422afad1-a409-4889-9cfc-365b0323e7c2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_422afad1-a409-4889-9cfc-365b0323e7c2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Symbols\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"H\",\n          \"G\",\n          \"S\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Meaning\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Hole\",\n          \"Goal\",\n          \"Start\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Question:** Explain what the objective of the agent is in this environment?\n",
        "\n",
        "\n",
        "## **Answer:**\n",
        "\n",
        "\n",
        "\n",
        "In the FrozenLake environment, the agent's objective is to navigate from the starting point (S) to the goal point (G) while avoiding falling into the holes (H). The agent receives a reward of 1 upon successfully reaching the goal and a reward of 0 if it falls into a hole.\n",
        "\n",
        "- The agent must learn to navigate the environment in such a way that it avoids falling into any of the holes. Falling into a hole results in an immediate loss and zero reward.\n",
        "\n",
        "- The agent's ultimate objective is to reach the goal point while maximizing its reward. Upon reaching the goal, the agent receives a reward of 1, indicating successful completion of the task.\n",
        "\n",
        "- Along the path from the starting point to the goal, the agent should aim to accumulate as much reward as possible by making efficient and safe moves.\n",
        "\n",
        "*https://www.gymlibrary.dev/environments/toy_text/frozen_lake/*"
      ],
      "metadata": {
        "id": "gskHuc9pGlah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------\n",
        "## **Practical:** Code up an AI that will employ random action selection in order to drive the agent. Test this random action selection agent with the above environment (i.e., code up a loop as I did above, but instead of taking input from a human user, take it from the AI you coded).\n"
      ],
      "metadata": {
        "id": "I8ZD4SyiHtre"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRwGwPoqw0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "81f2d1de-246f-4969-d0dd-107208aa963f"
      },
      "source": [
        "env.reset()\n",
        "# Run the episode until termination\n",
        "while True:\n",
        "  env.render()\n",
        "\n",
        "  # Select a random action from the action space\n",
        "  # (https://blog.paperspace.com/getting-started-with-openai-gym/)\n",
        "\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  observation, reward, done, info = env.step(action)\n",
        "\n",
        "  # Print the result of taking the action\n",
        "  print(\"--> The result of taking action\", action, \"is:\")\n",
        "  print(\"     S=\", observation)\n",
        "  print(\"     R=\", reward)\n",
        "  print(\"     Done=\", done)\n",
        "\n",
        "  # If the episode is done, break out of the loop\n",
        "  if done:\n",
        "        # If the agent fell into a hole\n",
        "    if reward == 0:\n",
        "        print(\"Agent fell into a hole.\")\n",
        "    # If the agent reached the target\n",
        "    elif reward == 1:\n",
        "        print(\"Agent reached the target.\")\n",
        "    break\n",
        "\n",
        "  env.render()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 2 is:\n",
            "     S= 2\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 3 is:\n",
            "     S= 2\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 3 is:\n",
            "     S= 2\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 3 is:\n",
            "     S= 2\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 0 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 3 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     Done= False\n",
            "--> The result of taking action 1 is:\n",
            "     S= 5\n",
            "     R= 0.0\n",
            "     Done= True\n",
            "Agent fell into a hole.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQLKuubzrl4L"
      },
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n",
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "#\n",
        "# (Optional): Repeat the above for q.\n",
        "#\n",
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?\n",
        "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps.\n",
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts.\n",
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "#\n",
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "# env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------\n",
        "## **Question:** How would you represent the agent's policy function and value function?\n",
        "\n",
        "## **Answer:**\n",
        "\n",
        "- We start with a set of probabilities $p(s',r|s,a)$, which represent the dynamics of the environment, indicating the probability of transitioning to state $s'$ and receiving reward $r$ given action $a$ in state $s$.\n",
        "\n",
        "- Using the Bellman equation for the state value function $v_{𝜋}$, we iteratively update the value of each state based on the current policy $𝜋$:\n",
        "\n",
        "  ($v_{k+1}(s)=\\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{k}(s')]$).\n",
        "\n",
        "- At each iteration, we estimate the expected return from each state by considering the possible actions, transitions to next states, and rewards, weighted by the policy probabilities and the transition dynamics.\n",
        "\n",
        "- The resulting approximate value function $v_{k+1}$ provides an estimate of the expected return from each state under the current policy  $𝜋$"
      ],
      "metadata": {
        "id": "iwLljKEnLjpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **Practical:** revise the above AI solver to use a policy function in which you code the random action selections in the policy function. Test this.\n",
        "\n"
      ],
      "metadata": {
        "id": "7dwzUC8XPBoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def random_action_policy(stateSpaceSize, actionSpaceSize):\n",
        "    return np.ones([stateSpaceSize, actionSpaceSize]) / actionSpaceSize\n",
        "\n",
        "\n",
        "env.reset()\n",
        "while True:\n",
        "    env.render()\n",
        "\n",
        "    # Get the current observation (state) from the environment\n",
        "    observation = env.env.s\n",
        "\n",
        "    # Generate a random action policy for the current state space and action space\n",
        "    policy = random_action_policy(env.observation_space.n, env.action_space.n)\n",
        "\n",
        "    # Choose action according to the generated policy\n",
        "    action = np.random.choice(env.action_space.n, p=policy[observation])\n",
        "\n",
        "    # Take the chosen action in the environment\n",
        "    (observation, reward, done, probability) = env.step(action)\n",
        "\n",
        "    # Print the result of taking the action\n",
        "    print(\"--> The result of taking action\", action, \"is:\")\n",
        "    print(\"     S=\", observation)\n",
        "    print(\"     R=\", reward)\n",
        "    print(\"     p=\", probability)\n",
        "\n",
        "    if done and reward == 1:\n",
        "        print(\"Goal Reached\")\n",
        "        break\n",
        "    if done and reward == 0:\n",
        "        print(\"Agent fell into Hole\")\n",
        "        break\n",
        "\n",
        "    # Render the updated state of the environment\n",
        "    env.render()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "M6u2xxN2LjR7",
        "outputId": "7325d445-7489-4720-988a-18151ca1350a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "--> The result of taking action 1 is:\n",
            "     S= 12\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Agent fell into Hole\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Practical**: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use either the inplace or ping-pong buffer (as described in the lecture). Now randomly initialize your policy function, and compute its value function. Report your results: policy and value function. Ensure your prediction algo reports how many iterations it took."
      ],
      "metadata": {
        "id": "gMoahtLRRJ4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_policy(env):\n",
        "    return np.random.randint(env.action_space.n, size=env.observation_space.n)\n",
        "\n",
        "def policy_evaluation(env, policy, gamma=0.5, theta=1e-6):\n",
        "\n",
        "    V_current = np.zeros(env.observation_space.n)\n",
        "    V_new = np.zeros(env.observation_space.n)\n",
        "\n",
        "    num_iterations = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V_current[s]\n",
        "            action = policy[s]\n",
        "            transition_probs = env.P[s][action]\n",
        "\n",
        "            # Calculate the new value of the state using the Bellman\n",
        "            v_new = sum(prob * (reward + gamma * V_current[next_state]) for prob, next_state, reward, _ in transition_probs)\n",
        "\n",
        "            # Update the maximum change in the value function\n",
        "            delta = max(delta, abs(v - v_new))\n",
        "\n",
        "            # the new buffer\n",
        "            V_new[s] = v_new\n",
        "\n",
        "        # Swap the buffers\n",
        "        V_current, V_new = V_new, V_current\n",
        "\n",
        "        num_iterations += 1  # Increment iteration counter\n",
        "\n",
        "        # convergence Check\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V_current, num_iterations\n",
        "\n",
        "\n",
        "\n",
        "env.reset()\n",
        "\n",
        "policy = random_policy(env)\n",
        "\n",
        "value_function, num_iterations = policy_evaluation(env, policy)\n",
        "\n",
        "print(\"\\nValue Function:\")\n",
        "print(value_function)\n",
        "print(\"\\nNumber of Iterations:\", num_iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZMjUmT_CRJnu",
        "outputId": "e0b49beb-9c1d-4b95-b905-be0c0d4bc2a0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Value Function:\n",
            "[1.62904666e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 8.15210584e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 4.07666300e-03 2.36452599e-02 6.89654870e-02 0.00000000e+00\n",
            " 0.00000000e+00 7.29063367e-02 4.13793055e-01 0.00000000e+00]\n",
            "\n",
            "Number of Iterations: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "## **(Optional):** Repeat the above for q."
      ],
      "metadata": {
        "id": "KvaJFea3TTsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_value_iteration(env, gamma=1.0, theta=1e-6):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    num_iterations = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.observation_space.n):\n",
        "            for a in range(env.action_space.n):\n",
        "                q = Q[s][a]\n",
        "\n",
        "                # Update Q-values using Bellman\n",
        "                next_states = env.P[s][a]\n",
        "\n",
        "                new_q = sum(prob * (reward + gamma * max(Q[next_state])) for prob, next_state, reward, _ in next_states)\n",
        "                Q[s][a] = new_q\n",
        "\n",
        "                delta = max(delta, abs(q - Q[s][a]))\n",
        "\n",
        "        num_iterations += 1\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return Q, num_iterations\n",
        "\n",
        "env.reset()\n",
        "\n",
        "Q_values, num_iterations = q_value_iteration(env)\n",
        "\n",
        "# Results\n",
        "print(\"Q-values:\")\n",
        "print(Q_values)\n",
        "print(\"\\nNumber of Iterations:\", num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FZXeQCqmQtUf",
        "outputId": "11039e0f-5994-4323-baf5-6bed0740a613"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values:\n",
            "[[0.82351228 0.82351054 0.82351054 0.82351014]\n",
            " [0.54900605 0.5490047  0.54900256 0.82350666]\n",
            " [0.72546939 0.7254687  0.72546709 0.82350275]\n",
            " [0.54900084 0.54900084 0.54899985 0.82350076]\n",
            " [0.823514   0.54901009 0.54900951 0.54900876]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.52940008 0.25489917 0.52940008 0.27450092]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.54901009 0.54901203 0.54901128 0.8235167 ]\n",
            " [0.568621   0.82352016 0.5490146  0.52940473]\n",
            " [0.76469778 0.58823108 0.49019106 0.45097341]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.56862215 0.60783979 0.88234651 0.58823108]\n",
            " [0.86273913 0.9411732  0.90195699 0.8823481 ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Number of Iterations: 319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Policy Improvement:**\n",
        "\n",
        "**Question:** How would you use P and your value function to improve an arbitrary policy, pi, per Chapter 4?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- Compute the action-value function $q_{\\pi}(s, a)$ for all actions a using the Bellman equation: $q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{\\pi}(s')]$\n",
        "\n",
        "- Update the policy $\\pi'(s)$ by selecting the action that maximizes the action-value function: $\\pi'(s) = \\arg\\max_{a} q_{\\pi}(s, a)$\n",
        "\n",
        "- Compute the value function $v_{\\pi'}(s)$ for the current policy $\\pi'$ using the Bellman expectation equation:\n",
        "$v_{\\pi'}(s) = \\sum_{a} \\pi'(a|s) \\sum_{s', r} p(s', r|s, a) [r + \\gamma v_{\\pi'}(s')]$\n",
        "\n",
        "- Improve the policy using the value function to obtain a new policy\n",
        "- Update the value function $v$ using the Bellman optimality equation:"
      ],
      "metadata": {
        "id": "rhnGDwctUAJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **Practical**: Code the policy iteration process, and employ it to arrive at a policy that solves this problem. Show your testing results, and ensure it reports the number of iterations for each step: (a) overall policy iteration steps and (b) evaluation steps."
      ],
      "metadata": {
        "id": "qXmM2E_QZ2_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, gamma=1.0, theta=1e-6, max_iterations=10000):\n",
        "    num_states = env.observation_space.n\n",
        "    value_function = np.zeros(num_states)\n",
        "    num_iterations = 0\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0\n",
        "        for state in range(num_states):\n",
        "            v = value_function[state]\n",
        "            action = policy[state]\n",
        "            state_value = 0\n",
        "            for prob, next_state, reward, _ in env.P[state][action]:\n",
        "                state_value += prob * (reward + gamma * value_function[next_state])\n",
        "            value_function[state] = state_value\n",
        "            delta = max(delta, abs(v - value_function[state]))\n",
        "        num_iterations += 1\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return value_function, num_iterations\n",
        "\n",
        "def policy_improvement(env, value_function, gamma=1.0):\n",
        "\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    policy = np.zeros(num_states, dtype=int)\n",
        "\n",
        "    for state in range(num_states):\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for action in range(num_actions):\n",
        "            for prob, next_state, reward, _ in env.P[state][action]:\n",
        "                action_values[action] += prob * (reward + gamma * value_function[next_state])\n",
        "        best_action = np.argmax(action_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy\n",
        "\n",
        "def policy_iteration(env, gamma=1.0, theta=1e-6, max_iterations=10000):\n",
        "\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Initialize a random policy\n",
        "    policy = np.random.randint(0, num_actions, num_states)\n",
        "    num_policy_iterations = 0\n",
        "    num_evaluation_iterations = 0\n",
        "\n",
        "    while True:\n",
        "        num_policy_iterations += 1\n",
        "\n",
        "        # Policy Evaluation\n",
        "        value_function, num_iterations = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
        "        num_evaluation_iterations += num_iterations\n",
        "\n",
        "        # Policy Improvement\n",
        "        new_policy = policy_improvement(env, value_function, gamma)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.array_equal(new_policy, policy):\n",
        "            break\n",
        "\n",
        "        policy = new_policy\n",
        "\n",
        "    return policy, num_policy_iterations, num_evaluation_iterations\n",
        "\n",
        "env.reset()\n",
        "\n",
        "# Perform policy iteration\n",
        "optimal_policy, num_policy_iterations, num_evaluation_iterations = policy_iteration(env)\n",
        "\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "print(\"Number of Policy Iterations:\", num_policy_iterations)\n",
        "print(\"Number of Evaluation Iterations:\", num_evaluation_iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sqptSlLjTk-r",
        "outputId": "2352f8e6-7047-418c-8124-dc396e242912"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: [0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "Number of Policy Iterations: 4\n",
            "Number of Evaluation Iterations: 864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Practical**: Code the value iteration process, and employ it to arrive at a policy that solves this problem. Show your testing results, reporting the iteration counts."
      ],
      "metadata": {
        "id": "iG0csda6a9kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma=1.0, theta=1e-6, max_iterations=10000):\n",
        "\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    value_function = np.zeros(num_states)\n",
        "    num_iterations = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(num_states):\n",
        "            v = value_function[state]\n",
        "            action_values = np.zeros(num_actions)\n",
        "            for action in range(num_actions):\n",
        "                for prob, next_state, reward, _ in env.P[state][action]:\n",
        "                    action_values[action] += prob * (reward + gamma * value_function[next_state])\n",
        "            value_function[state] = np.max(action_values)\n",
        "            delta = max(delta, np.abs(v - value_function[state]))\n",
        "        num_iterations += 1\n",
        "        if delta < theta or num_iterations >= max_iterations:\n",
        "            break\n",
        "\n",
        "    # Extract the optimal policy\n",
        "    optimal_policy = np.zeros(num_states, dtype=int)\n",
        "    for state in range(num_states):\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for action in range(num_actions):\n",
        "            for prob, next_state, reward, _ in env.P[state][action]:\n",
        "                action_values[action] += prob * (reward + gamma * value_function[next_state])\n",
        "        optimal_policy[state] = np.argmax(action_values)\n",
        "\n",
        "    return optimal_policy, num_iterations\n",
        "\n",
        "env.reset()\n",
        "\n",
        "optimal_policy, num_iterations = value_iteration(env)\n",
        "\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "print(\"Number of Iterations:\", num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "irO1KxyGapWL",
        "outputId": "be989ce5-7034-4f68-c3ac-15045dbed34f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: [0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "Number of Iterations: 321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## Comment on the difference between the iterations required for policy vs value iteration.\n",
        "\n",
        "\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- Policy Iteration: Alternates between policy evaluation and improvement. Each policy iteration involves a complete evaluation of the current policy, which can lead to a higher number of iterations.\n",
        "- Value Iteration: Directly computes the optimal value function by iteratively updating the value estimates for each state until convergence. It typically requires fewer iterations compared to policy iteration since it does not involve separate policy evaluation steps.\n",
        "- In the results, policy iteration required 864 evaluations, while value iteration required 321 iterations to converge to the optimal policy.\n"
      ],
      "metadata": {
        "id": "LijYuI0Pc8tz"
      }
    }
  ]
}